{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gudimetlatejaswi/Electric-Vehicle-Data-Analysis/blob/main/code3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuGPqBURNMkE"
      },
      "outputs": [],
      "source": [
        "!pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpKY2jHnd4tc"
      },
      "outputs": [],
      "source": [
        "pip install pymuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAGU33HLeGZK"
      },
      "outputs": [],
      "source": [
        "pip install fitz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW_soFydFUcZ",
        "outputId": "cdc943be-6cf4-4fda-fa47-129cc9523406"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://05ffaf5335ba4bde55.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Adaptive Property Extractor - Zero Hardcoded Values\n",
        "# Learns everything from your documents dynamically\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def setup_colab_environment():\n",
        "    \"\"\"Setup Colab environment safely\"\"\"\n",
        "    try:\n",
        "        # Install packages without version conflicts\n",
        "        packages = [\"gradio\", \"requests\", \"Pillow\", \"pandas\", \"numpy\"]\n",
        "        for package in packages:\n",
        "            try:\n",
        "                __import__(package.replace(\"-\", \"_\"))\n",
        "            except ImportError:\n",
        "                subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package],\n",
        "                             capture_output=True, text=True)\n",
        "\n",
        "        # Handle PyMuPDF separately\n",
        "        try:\n",
        "            import fitz\n",
        "        except ImportError:\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"PyMuPDF==1.23.26\"],\n",
        "                         capture_output=True, text=True)\n",
        "            import fitz\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Setup warning: {e}\")\n",
        "        return False\n",
        "\n",
        "setup_colab_environment()\n",
        "\n",
        "# Import required modules\n",
        "import re\n",
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, List, Any, Tuple, Optional\n",
        "from collections import defaultdict, Counter\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "import base64\n",
        "import io\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "try:\n",
        "    import fitz\n",
        "    PDF_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PDF_AVAILABLE = False\n",
        "    print(\"PDF processing may be limited\")\n",
        "\n",
        "# API Configuration\n",
        "GEMINI_API_KEY = \"AIzaSyCFzlJFsIq6PYLuHSPqLYvg0clx-CPpSD0\"\n",
        "GEMINI_ENDPOINT = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent\"\n",
        "\n",
        "@dataclass\n",
        "class AdaptiveCandidate:\n",
        "    \"\"\"Candidate with learned characteristics\"\"\"\n",
        "    value: float\n",
        "    unit: str\n",
        "    context: str\n",
        "    full_context: str\n",
        "    page: int\n",
        "    context_indicators: List[str]\n",
        "    data_quality_indicators: List[str]\n",
        "    surrounding_values: List[Tuple[float, str]]  # Nearby values for context\n",
        "    signature: str\n",
        "    is_assigned: bool = False\n",
        "\n",
        "@dataclass\n",
        "class LearnedProperty:\n",
        "    \"\"\"Property learned from Excel template\"\"\"\n",
        "    name: str\n",
        "    category: str\n",
        "    row: int\n",
        "    learned_characteristics: Dict[str, Any]\n",
        "    contextual_hints: List[str]\n",
        "    semantic_profile: Dict[str, float]\n",
        "\n",
        "class AdaptivePropertyExtractor:\n",
        "    \"\"\"Completely adaptive extractor that learns from documents\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.universal_unit_patterns = self._build_universal_patterns()\n",
        "        self.learned_document_profile = {}\n",
        "        self.learned_unit_contexts = defaultdict(list)\n",
        "        self.learned_value_distributions = defaultdict(list)\n",
        "        self.assigned_candidates = set()\n",
        "\n",
        "    def _build_universal_patterns(self):\n",
        "        \"\"\"Universal patterns - only detect format, not meaning\"\"\"\n",
        "        return {\n",
        "            # Find any number followed by common unit symbols\n",
        "            'any_unit': re.compile(r'(\\d+\\.?\\d*(?:[eE][+-]?\\d+)?)\\s+([A-Za-z][A-Za-z²³⁻¹°/]*)', re.IGNORECASE)\n",
        "        }\n",
        "\n",
        "    def extract_pdf_and_learn(self, pdf_file) -> Tuple[str, Dict, Dict]:\n",
        "        \"\"\"Extract PDF and learn document characteristics\"\"\"\n",
        "        if not PDF_AVAILABLE:\n",
        "            return \"PDF processing unavailable\", {}, {}\n",
        "\n",
        "        try:\n",
        "            doc = fitz.open(pdf_file.name)\n",
        "            full_text = \"\"\n",
        "            images_data = {}\n",
        "\n",
        "            # Document learning\n",
        "            document_profile = {\n",
        "                'units_found': set(),\n",
        "                'value_contexts': defaultdict(list),\n",
        "                'numerical_patterns': defaultdict(list),\n",
        "                'context_types': Counter(),\n",
        "                'semantic_indicators': defaultdict(set)\n",
        "            }\n",
        "\n",
        "            print(f\"Learning from {doc.page_count} pages...\")\n",
        "\n",
        "            for page_num, page in enumerate(doc):\n",
        "                page_text = page.get_text()\n",
        "                if page_text.strip():\n",
        "                    full_text += f\"\\n--- PAGE {page_num+1} ---\\n{page_text}\"\n",
        "                    self._learn_from_page(page_text, page_num + 1, document_profile)\n",
        "\n",
        "                # Extract images\n",
        "                images = page.get_images(full=True)\n",
        "                if images:\n",
        "                    try:\n",
        "                        img = images[0]\n",
        "                        xref = img[0]\n",
        "                        base_image = doc.extract_image(xref)\n",
        "                        image_bytes = base_image[\"image\"]\n",
        "\n",
        "                        pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "                        buffered = io.BytesIO()\n",
        "                        pil_image.save(buffered, format=\"PNG\", quality=85)\n",
        "                        img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
        "\n",
        "                        images_data[f\"page_{page_num+1}\"] = {\n",
        "                            \"base64\": img_base64,\n",
        "                            \"format\": \"image/png\",\n",
        "                            \"page\": page_num+1\n",
        "                        }\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "            doc.close()\n",
        "\n",
        "            # Learn document characteristics\n",
        "            self.learned_document_profile = self._analyze_learned_patterns(document_profile)\n",
        "\n",
        "            return full_text, images_data, document_profile\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error: {str(e)}\", {}, {}\n",
        "\n",
        "    def _learn_from_page(self, page_text: str, page_num: int, profile: Dict):\n",
        "        \"\"\"Learn patterns from each page\"\"\"\n",
        "        lines = page_text.split('\\n')\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            line_clean = line.strip()\n",
        "            if not line_clean:\n",
        "                continue\n",
        "\n",
        "            # Find all numbers with units using universal pattern\n",
        "            for match in self.universal_unit_patterns['any_unit'].finditer(line_clean):\n",
        "                try:\n",
        "                    value_str = match.group(1)\n",
        "                    unit_str = match.group(2).strip()\n",
        "                    value = float(value_str.replace('×', 'e').replace('−', '-'))\n",
        "\n",
        "                    if value > 0:\n",
        "                        # Learn about this unit\n",
        "                        profile['units_found'].add(unit_str)\n",
        "                        profile['value_contexts'][unit_str].append({\n",
        "                            'value': value,\n",
        "                            'context': line_clean,\n",
        "                            'page': page_num,\n",
        "                            'surrounding_context': self._get_surrounding_context(lines, i)\n",
        "                        })\n",
        "                        profile['numerical_patterns'][unit_str].append(value)\n",
        "\n",
        "                        # Learn semantic indicators around this unit\n",
        "                        context_words = self._extract_semantic_words(line_clean)\n",
        "                        for word in context_words:\n",
        "                            profile['semantic_indicators'][unit_str].add(word)\n",
        "\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            # Learn context types\n",
        "            if re.search(r'table\\s+\\d+', line_clean, re.IGNORECASE):\n",
        "                profile['context_types']['table'] += 1\n",
        "            elif re.search(r'fig(?:ure)?\\s+\\d+', line_clean, re.IGNORECASE):\n",
        "                profile['context_types']['figure'] += 1\n",
        "            elif any(word in line_clean.lower() for word in ['test', 'measured', 'result']):\n",
        "                profile['context_types']['experimental'] += 1\n",
        "\n",
        "    def _get_surrounding_context(self, lines: List[str], center: int) -> str:\n",
        "        \"\"\"Get surrounding context for learning\"\"\"\n",
        "        start = max(0, center - 2)\n",
        "        end = min(len(lines), center + 3)\n",
        "        return ' '.join([line.strip() for line in lines[start:end] if line.strip()])\n",
        "\n",
        "    def _extract_semantic_words(self, context: str) -> List[str]:\n",
        "        \"\"\"Extract meaningful words from context\"\"\"\n",
        "        # Find words that might indicate property types\n",
        "        semantic_words = []\n",
        "        context_lower = context.lower()\n",
        "\n",
        "        # Technical terms that often appear near property values\n",
        "        technical_terms = [\n",
        "            'modulus', 'strength', 'stress', 'strain', 'yield', 'ultimate', 'tensile',\n",
        "            'flexural', 'compression', 'shear', 'elastic', 'density', 'temperature',\n",
        "            'energy', 'toughness', 'hardness', 'poisson', 'ratio', 'elongation'\n",
        "        ]\n",
        "\n",
        "        for term in technical_terms:\n",
        "            if term in context_lower:\n",
        "                semantic_words.append(term)\n",
        "\n",
        "        return semantic_words\n",
        "\n",
        "    def _analyze_learned_patterns(self, document_profile: Dict) -> Dict:\n",
        "        \"\"\"Analyze learned patterns to create adaptive rules\"\"\"\n",
        "        analyzed = {\n",
        "            'unit_characteristics': {},\n",
        "            'value_ranges': {},\n",
        "            'context_associations': {},\n",
        "            'semantic_patterns': {}\n",
        "        }\n",
        "\n",
        "        # Analyze each discovered unit\n",
        "        for unit, contexts in document_profile['value_contexts'].items():\n",
        "            if len(contexts) < 2:  # Need multiple examples to learn\n",
        "                continue\n",
        "\n",
        "            values = [ctx['value'] for ctx in contexts]\n",
        "            semantic_words = document_profile['semantic_indicators'][unit]\n",
        "\n",
        "            # Learn unit characteristics\n",
        "            analyzed['unit_characteristics'][unit] = {\n",
        "                'occurrence_count': len(contexts),\n",
        "                'value_range': (min(values), max(values)),\n",
        "                'typical_magnitude': np.median(values),\n",
        "                'associated_contexts': list(semantic_words),\n",
        "                'appears_with_properties': self._infer_property_associations(semantic_words)\n",
        "            }\n",
        "\n",
        "            # Learn value distributions\n",
        "            analyzed['value_ranges'][unit] = {\n",
        "                'min_observed': min(values),\n",
        "                'max_observed': max(values),\n",
        "                'median': np.median(values),\n",
        "                'std': np.std(values) if len(values) > 1 else 0\n",
        "            }\n",
        "\n",
        "        return analyzed\n",
        "\n",
        "    def _infer_property_associations(self, semantic_words: set) -> List[str]:\n",
        "        \"\"\"Infer what property types this unit is associated with\"\"\"\n",
        "        associations = []\n",
        "\n",
        "        # Learn associations from semantic context\n",
        "        if 'modulus' in semantic_words:\n",
        "            associations.append('modulus_type')\n",
        "        if any(word in semantic_words for word in ['strength', 'stress']):\n",
        "            associations.append('strength_type')\n",
        "        if any(word in semantic_words for word in ['strain', 'elongation']):\n",
        "            associations.append('strain_type')\n",
        "        if 'energy' in semantic_words:\n",
        "            associations.append('energy_type')\n",
        "        if any(word in semantic_words for word in ['density', 'mass']):\n",
        "            associations.append('density_type')\n",
        "\n",
        "        return associations\n",
        "\n",
        "    def read_excel_and_learn(self, excel_file) -> List[LearnedProperty]:\n",
        "        \"\"\"Read Excel and learn property characteristics\"\"\"\n",
        "        try:\n",
        "            # Flexible Excel reading\n",
        "            df = None\n",
        "            for header in [None, 0, 1]:\n",
        "                try:\n",
        "                    df = pd.read_excel(excel_file, header=header)\n",
        "                    if len(df) >= 3:\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if df is None:\n",
        "                return []\n",
        "\n",
        "            learned_properties = []\n",
        "            current_category = \"General\"\n",
        "\n",
        "            for index, row in df.iterrows():\n",
        "                property_name = self._extract_property_name_adaptive(row)\n",
        "                if not property_name or self._is_non_property(property_name):\n",
        "                    continue\n",
        "\n",
        "                # Learn category adaptively\n",
        "                category = self._learn_category(property_name, current_category)\n",
        "                if category != current_category:\n",
        "                    current_category = category\n",
        "\n",
        "                # Learn property characteristics from name\n",
        "                characteristics = self._learn_property_characteristics(property_name, row)\n",
        "\n",
        "                learned_property = LearnedProperty(\n",
        "                    name=property_name,\n",
        "                    category=category,\n",
        "                    row=index + 1,\n",
        "                    learned_characteristics=characteristics,\n",
        "                    contextual_hints=self._extract_contextual_hints(property_name, row),\n",
        "                    semantic_profile=self._build_semantic_profile(property_name)\n",
        "                )\n",
        "\n",
        "                learned_properties.append(learned_property)\n",
        "\n",
        "            print(f\"Learned {len(learned_properties)} property profiles\")\n",
        "            return learned_properties\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Excel learning error: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _extract_property_name_adaptive(self, row) -> Optional[str]:\n",
        "        \"\"\"Adaptively extract property name from any Excel format\"\"\"\n",
        "        candidates = []\n",
        "\n",
        "        for col in range(min(len(row), 8)):\n",
        "            if pd.notna(row.iloc[col]):\n",
        "                value = str(row.iloc[col]).strip()\n",
        "                if len(value) > 2:\n",
        "                    candidates.append(value)\n",
        "\n",
        "        # Choose the most likely property name\n",
        "        for candidate in candidates:\n",
        "            if (len(candidate) > 3 and\n",
        "                not re.match(r'^\\d+\\.?\\d*$', candidate) and  # Not just numbers\n",
        "                not candidate.lower() in ['description', 'value', 'unit', 'note']):\n",
        "                return candidate\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _is_non_property(self, name: str) -> bool:\n",
        "        \"\"\"Determine if this is not a property using learned patterns\"\"\"\n",
        "        name_lower = name.lower()\n",
        "\n",
        "        # Learn from patterns that indicate non-properties\n",
        "        non_property_indicators = [\n",
        "            'description', 'value', 'unit', 'note', 'header', 'title',\n",
        "            'section', 'chapter', 'page', 'figure', 'table', 'equation'\n",
        "        ]\n",
        "\n",
        "        return (any(indicator in name_lower for indicator in non_property_indicators) or\n",
        "                len(name) < 3 or\n",
        "                re.match(r'^[A-Z\\s]+$', name))  # All caps headers\n",
        "\n",
        "    def _learn_category(self, property_name: str, current_category: str) -> str:\n",
        "        \"\"\"Learn category from property name semantics\"\"\"\n",
        "        prop = property_name.lower()\n",
        "\n",
        "        # Learn category patterns from semantic content\n",
        "        category_patterns = {\n",
        "            'Mechanical': ['tensile', 'flexural', 'compression', 'shear', 'modulus', 'strength', 'stress', 'strain', 'elastic'],\n",
        "            'Impact': ['impact', 'energy', 'fracture', 'toughness', 'izod', 'charpy'],\n",
        "            'Thermal': ['temperature', 'thermal', 'heat', 'melting', 'glass', 'transition'],\n",
        "            'Physical': ['density', 'hardness', 'mass', 'weight', 'color', 'appearance'],\n",
        "            'Chemical': ['concentration', 'ph', 'viscosity', 'composition', 'molecular'],\n",
        "            'Electrical': ['voltage', 'current', 'resistance', 'conductivity', 'dielectric']\n",
        "        }\n",
        "\n",
        "        # Score each category based on semantic overlap\n",
        "        best_category = current_category\n",
        "        best_score = 0\n",
        "\n",
        "        for category, indicators in category_patterns.items():\n",
        "            score = sum(1 for indicator in indicators if indicator in prop)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_category = category\n",
        "\n",
        "        return best_category\n",
        "\n",
        "    def _learn_property_characteristics(self, property_name: str, row) -> Dict[str, Any]:\n",
        "        \"\"\"Learn property characteristics from name and context\"\"\"\n",
        "        characteristics = {\n",
        "            'semantic_keywords': [],\n",
        "            'likely_measurement_type': 'unknown',\n",
        "            'expected_magnitude_class': 'unknown',\n",
        "            'unit_hints': [],\n",
        "            'property_type_indicators': []\n",
        "        }\n",
        "\n",
        "        prop_lower = property_name.lower()\n",
        "\n",
        "        # Extract semantic keywords\n",
        "        keywords = [word for word in prop_lower.split() if len(word) > 2]\n",
        "        characteristics['semantic_keywords'] = keywords\n",
        "\n",
        "        # Learn measurement type from semantics\n",
        "        if any(word in prop_lower for word in ['modulus', 'stiffness']):\n",
        "            characteristics['likely_measurement_type'] = 'stiffness'\n",
        "            characteristics['expected_magnitude_class'] = 'large_values'\n",
        "        elif any(word in prop_lower for word in ['strength', 'stress']):\n",
        "            characteristics['likely_measurement_type'] = 'stress'\n",
        "            characteristics['expected_magnitude_class'] = 'medium_values'\n",
        "        elif any(word in prop_lower for word in ['strain', 'elongation', 'deformation']):\n",
        "            characteristics['likely_measurement_type'] = 'deformation'\n",
        "            characteristics['expected_magnitude_class'] = 'small_values'\n",
        "        elif any(word in prop_lower for word in ['energy', 'toughness']):\n",
        "            characteristics['likely_measurement_type'] = 'energy'\n",
        "            characteristics['expected_magnitude_class'] = 'variable_values'\n",
        "        elif any(word in prop_lower for word in ['density', 'mass']):\n",
        "            characteristics['likely_measurement_type'] = 'density'\n",
        "            characteristics['expected_magnitude_class'] = 'low_values'\n",
        "        elif any(word in prop_lower for word in ['temperature', 'thermal']):\n",
        "            characteristics['likely_measurement_type'] = 'temperature'\n",
        "            characteristics['expected_magnitude_class'] = 'temperature_range'\n",
        "\n",
        "        # Learn unit hints from row data and property name\n",
        "        row_text = ' '.join([str(cell) for cell in row if pd.notna(cell)])\n",
        "        characteristics['unit_hints'] = self._extract_unit_hints(property_name + ' ' + row_text)\n",
        "\n",
        "        return characteristics\n",
        "\n",
        "    def _extract_unit_hints(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract unit hints from text\"\"\"\n",
        "        # Common unit indicators\n",
        "        common_units = ['GPa', 'MPa', 'Pa', '%', 'J/m²', 'J', '°C', 'K', 'g/cm³',\n",
        "                       'dimensionless', 's⁻¹', 'Hz', 'V', 'A', 'mol/L', 'ppm']\n",
        "\n",
        "        hints = []\n",
        "        for unit in common_units:\n",
        "            if unit.lower() in text.lower():\n",
        "                hints.append(unit)\n",
        "\n",
        "        return hints\n",
        "\n",
        "    def _build_semantic_profile(self, property_name: str) -> Dict[str, float]:\n",
        "        \"\"\"Build semantic profile for property matching\"\"\"\n",
        "        prop_lower = property_name.lower()\n",
        "\n",
        "        # Semantic weight distribution (learned from property name)\n",
        "        semantic_weights = {}\n",
        "\n",
        "        words = prop_lower.split()\n",
        "        for word in words:\n",
        "            if len(word) > 2:\n",
        "                semantic_weights[word] = 1.0 / len(words)  # Distribute weight evenly\n",
        "\n",
        "        # Boost important semantic indicators\n",
        "        if 'modulus' in prop_lower:\n",
        "            semantic_weights['modulus'] = semantic_weights.get('modulus', 0) + 0.5\n",
        "        if 'strength' in prop_lower:\n",
        "            semantic_weights['strength'] = semantic_weights.get('strength', 0) + 0.5\n",
        "        if 'strain' in prop_lower:\n",
        "            semantic_weights['strain'] = semantic_weights.get('strain', 0) + 0.5\n",
        "\n",
        "        return semantic_weights\n",
        "\n",
        "    def _extract_contextual_hints(self, property_name: str, row) -> List[str]:\n",
        "        \"\"\"Extract contextual hints from Excel row\"\"\"\n",
        "        hints = []\n",
        "\n",
        "        # Add property name components\n",
        "        hints.extend([word for word in property_name.lower().split() if len(word) > 2])\n",
        "\n",
        "        # Add hints from other cells in row\n",
        "        for cell in row:\n",
        "            if pd.notna(cell):\n",
        "                cell_str = str(cell).lower()\n",
        "                if len(cell_str) > 2 and cell_str != property_name.lower():\n",
        "                    hints.append(cell_str)\n",
        "\n",
        "        return list(set(hints))\n",
        "\n",
        "    def discover_all_candidates_adaptively(self, pdf_text: str) -> List[AdaptiveCandidate]:\n",
        "        \"\"\"Discover candidates using learned patterns\"\"\"\n",
        "        candidates = []\n",
        "        lines = pdf_text.split('\\n')\n",
        "\n",
        "        print(f\"Adaptive candidate discovery from {len(lines)} lines...\")\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            line_clean = line.strip()\n",
        "            if not line_clean or len(line_clean) < 5:\n",
        "                continue\n",
        "\n",
        "            # Skip obvious structural elements (learned adaptively)\n",
        "            if self._is_structural_element(line_clean):\n",
        "                continue\n",
        "\n",
        "            page_num = self._extract_page_number(lines[:i])\n",
        "            full_context = self._build_rich_context(lines, i)\n",
        "\n",
        "            # Analyze context characteristics\n",
        "            context_indicators = self._analyze_context_indicators(full_context)\n",
        "            data_quality = self._assess_data_quality_adaptive(full_context)\n",
        "            surrounding_values = self._find_surrounding_values(lines, i)\n",
        "\n",
        "            # Find numerical values adaptively\n",
        "            for match in self.universal_unit_patterns['any_unit'].finditer(line_clean):\n",
        "                try:\n",
        "                    value_str = match.group(1)\n",
        "                    unit_str = match.group(2).strip()\n",
        "                    value = float(value_str.replace('×', 'e').replace('−', '-'))\n",
        "\n",
        "                    if 0 < value < 1e10:\n",
        "                        # Create adaptive signature\n",
        "                        signature = f\"{value}_{unit_str}_{abs(hash(line_clean[:30]))}\"\n",
        "\n",
        "                        candidate = AdaptiveCandidate(\n",
        "                            value=value,\n",
        "                            unit=unit_str,\n",
        "                            context=line_clean,\n",
        "                            full_context=full_context,\n",
        "                            page=page_num,\n",
        "                            context_indicators=context_indicators,\n",
        "                            data_quality_indicators=data_quality,\n",
        "                            surrounding_values=surrounding_values,\n",
        "                            signature=signature\n",
        "                        )\n",
        "\n",
        "                        candidates.append(candidate)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        print(f\"Discovered {len(candidates)} adaptive candidates\")\n",
        "        return candidates\n",
        "\n",
        "    def _is_structural_element(self, line: str) -> bool:\n",
        "        \"\"\"Identify structural elements to exclude\"\"\"\n",
        "        # Learn patterns that indicate document structure\n",
        "        structural_patterns = [\n",
        "            r'^\\d+\\.\\s*[A-Z]',  # Section headers\n",
        "            r'^\\d+\\.\\d+\\s*[A-Z]',  # Subsection headers\n",
        "            r'^Abstract$|^Introduction$|^Conclusion$',  # Section titles\n",
        "            r'^\\s*\\[\\d+\\]',  # References\n",
        "            r'^Page \\d+|^\\d+$'  # Page numbers\n",
        "        ]\n",
        "\n",
        "        return any(re.match(pattern, line.strip(), re.IGNORECASE) for pattern in structural_patterns)\n",
        "\n",
        "    def _analyze_context_indicators(self, context: str) -> List[str]:\n",
        "        \"\"\"Analyze context to find indicators\"\"\"\n",
        "        indicators = []\n",
        "        context_lower = context.lower()\n",
        "\n",
        "        # Data structure indicators\n",
        "        if 'table' in context_lower:\n",
        "            indicators.append('tabular_context')\n",
        "        if any(term in context_lower for term in ['figure', 'fig.', 'graph']):\n",
        "            indicators.append('graphical_context')\n",
        "        if any(term in context_lower for term in ['measured', 'tested', 'experimental']):\n",
        "            indicators.append('measurement_context')\n",
        "        if any(term in context_lower for term in ['calculated', 'model', 'equation']):\n",
        "            indicators.append('theoretical_context')\n",
        "\n",
        "        # Property indicators\n",
        "        property_indicators = ['modulus', 'strength', 'stress', 'strain', 'yield', 'ultimate']\n",
        "        for indicator in property_indicators:\n",
        "            if indicator in context_lower:\n",
        "                indicators.append(f'property_{indicator}')\n",
        "\n",
        "        return indicators\n",
        "\n",
        "    def _assess_data_quality_adaptive(self, context: str) -> List[str]:\n",
        "        \"\"\"Assess data quality using learned patterns\"\"\"\n",
        "        quality_indicators = []\n",
        "        context_lower = context.lower()\n",
        "\n",
        "        # High quality indicators\n",
        "        if any(term in context_lower for term in ['table', 'data', 'results', 'measured']):\n",
        "            quality_indicators.append('high_quality_source')\n",
        "\n",
        "        # Medium quality indicators\n",
        "        if any(term in context_lower for term in ['figure', 'graph', 'chart']):\n",
        "            quality_indicators.append('medium_quality_source')\n",
        "\n",
        "        # Value density check\n",
        "        numbers_in_context = len(re.findall(r'\\d+\\.?\\d*', context))\n",
        "        words_in_context = len(context.split())\n",
        "\n",
        "        if words_in_context > 0 and (numbers_in_context / words_in_context) > 0.3:\n",
        "            quality_indicators.append('data_rich_context')\n",
        "\n",
        "        return quality_indicators\n",
        "\n",
        "    def _find_surrounding_values(self, lines: List[str], center: int) -> List[Tuple[float, str]]:\n",
        "        \"\"\"Find surrounding values for context learning\"\"\"\n",
        "        surrounding = []\n",
        "\n",
        "        start = max(0, center - 2)\n",
        "        end = min(len(lines), center + 3)\n",
        "\n",
        "        for line in lines[start:end]:\n",
        "            for match in self.universal_unit_patterns['any_unit'].finditer(line):\n",
        "                try:\n",
        "                    value = float(match.group(1).replace('×', 'e').replace('−', '-'))\n",
        "                    unit = match.group(2).strip()\n",
        "                    surrounding.append((value, unit))\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        return surrounding\n",
        "\n",
        "    def _build_rich_context(self, lines: List[str], center: int) -> str:\n",
        "        \"\"\"Build rich context for adaptive analysis\"\"\"\n",
        "        start = max(0, center - 3)\n",
        "        end = min(len(lines), center + 4)\n",
        "\n",
        "        context_lines = []\n",
        "        for line in lines[start:end]:\n",
        "            cleaned = line.strip()\n",
        "            if cleaned and len(cleaned) > 3:\n",
        "                context_lines.append(cleaned)\n",
        "\n",
        "        return ' | '.join(context_lines)\n",
        "\n",
        "    def _extract_page_number(self, lines: List[str]) -> int:\n",
        "        \"\"\"Extract page number adaptively\"\"\"\n",
        "        for line in reversed(lines[-8:]):\n",
        "            if '--- PAGE' in line:\n",
        "                match = re.search(r'PAGE (\\d+)', line)\n",
        "                if match:\n",
        "                    return int(match.group(1))\n",
        "        return 1\n",
        "\n",
        "    def match_adaptively_with_learning(self, properties: List[LearnedProperty],\n",
        "                                      candidates: List[AdaptiveCandidate]) -> Dict:\n",
        "        \"\"\"Match properties using learned patterns and adaptive scoring\"\"\"\n",
        "        results = {}\n",
        "\n",
        "        print(f\"Adaptive matching: {len(properties)} properties, {len(candidates)} candidates\")\n",
        "\n",
        "        # Learn optimal assignment using adaptive algorithm\n",
        "        for prop in properties:\n",
        "            best_candidate = None\n",
        "            best_score = 0\n",
        "\n",
        "            for candidate in candidates:\n",
        "                if candidate.is_assigned:\n",
        "                    continue\n",
        "\n",
        "                # Adaptive scoring based on learned patterns\n",
        "                score = self._calculate_adaptive_score(candidate, prop)\n",
        "\n",
        "                if score > best_score:\n",
        "                    best_candidate = candidate\n",
        "                    best_score = score\n",
        "\n",
        "            # Use adaptive threshold (learned from data distribution)\n",
        "            adaptive_threshold = self._calculate_adaptive_threshold(prop, candidates)\n",
        "\n",
        "            if best_candidate and best_score >= adaptive_threshold:\n",
        "                # Validate using learned constraints\n",
        "                if self._validate_with_learned_constraints(best_candidate, prop):\n",
        "                    best_candidate.is_assigned = True\n",
        "\n",
        "                    # Adaptive confidence assignment\n",
        "                    confidence = self._assign_adaptive_confidence(best_score, best_candidate, adaptive_threshold)\n",
        "\n",
        "                    results[prop.name] = {\n",
        "                        'value': f\"{best_candidate.value} {best_candidate.unit}\",\n",
        "                        'unit': best_candidate.unit,\n",
        "                        'source': f\"Page {best_candidate.page}\",\n",
        "                        'confidence': confidence,\n",
        "                        'adaptive_score': best_score,\n",
        "                        'adaptive_threshold': adaptive_threshold,\n",
        "                        'context': best_candidate.context[:80],\n",
        "                        'validation_status': 'adaptive_match',\n",
        "                        'learned_basis': 'document_patterns'\n",
        "                    }\n",
        "                else:\n",
        "                    results[prop.name] = {\n",
        "                        'value': 'N/A',\n",
        "                        'unit': 'N/A',\n",
        "                        'source': 'Failed adaptive validation',\n",
        "                        'confidence': 'none',\n",
        "                        'adaptive_score': best_score,\n",
        "                        'validation_status': 'rejected_by_learning'\n",
        "                    }\n",
        "            else:\n",
        "                results[prop.name] = {\n",
        "                    'value': 'N/A',\n",
        "                    'unit': 'N/A',\n",
        "                    'source': 'Below adaptive threshold',\n",
        "                    'confidence': 'none',\n",
        "                    'adaptive_score': best_score,\n",
        "                    'adaptive_threshold': adaptive_threshold,\n",
        "                    'validation_status': 'insufficient_adaptive_score'\n",
        "                }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _calculate_adaptive_score(self, candidate: AdaptiveCandidate, prop: LearnedProperty) -> float:\n",
        "        \"\"\"Calculate score using adaptive learning\"\"\"\n",
        "        score = 0\n",
        "\n",
        "        # Semantic matching based on learned profiles\n",
        "        semantic_matches = 0\n",
        "        for keyword, weight in prop.semantic_profile.items():\n",
        "            if keyword in candidate.full_context.lower():\n",
        "                semantic_matches += weight\n",
        "                score += 20 * weight\n",
        "\n",
        "        # Unit-property association learning\n",
        "        unit_compatibility = self._learn_unit_compatibility(candidate.unit, prop)\n",
        "        score += unit_compatibility * 30\n",
        "\n",
        "        # Context quality based on learned patterns\n",
        "        context_quality = self._assess_context_quality_learned(candidate, prop)\n",
        "        score += context_quality * 25\n",
        "\n",
        "        # Value reasonableness based on learned distributions\n",
        "        value_reasonableness = self._assess_value_reasonableness_learned(candidate)\n",
        "        score += value_reasonableness * 15\n",
        "\n",
        "        # Surrounding value consistency\n",
        "        consistency_score = self._assess_value_consistency(candidate)\n",
        "        score += consistency_score * 10\n",
        "\n",
        "        return min(score, 100)\n",
        "\n",
        "    def _learn_unit_compatibility(self, unit: str, prop: LearnedProperty) -> float:\n",
        "        \"\"\"Learn unit compatibility from document patterns\"\"\"\n",
        "        # Check if this unit appears in document with similar semantic context\n",
        "        if unit in self.learned_document_profile.get('unit_characteristics', {}):\n",
        "            unit_profile = self.learned_document_profile['unit_characteristics'][unit]\n",
        "            associated_properties = unit_profile.get('appears_with_properties', [])\n",
        "\n",
        "            prop_measurement_type = prop.learned_characteristics.get('likely_measurement_type', 'unknown')\n",
        "\n",
        "            # Learn compatibility from co-occurrence\n",
        "            type_mapping = {\n",
        "                'stiffness': 'modulus_type',\n",
        "                'stress': 'strength_type',\n",
        "                'deformation': 'strain_type',\n",
        "                'energy': 'energy_type',\n",
        "                'density': 'density_type'\n",
        "            }\n",
        "\n",
        "            expected_type = type_mapping.get(prop_measurement_type, 'unknown')\n",
        "\n",
        "            if expected_type in associated_properties:\n",
        "                return 1.0\n",
        "            elif associated_properties:  # Unit appears with other properties\n",
        "                return 0.3\n",
        "\n",
        "        # Fallback to unit hints from Excel\n",
        "        if unit in prop.learned_characteristics.get('unit_hints', []):\n",
        "            return 0.8\n",
        "\n",
        "        return 0.1  # Unknown compatibility\n",
        "\n",
        "    def _assess_context_quality_learned(self, candidate: AdaptiveCandidate, prop: LearnedProperty) -> float:\n",
        "        \"\"\"Assess context quality using learned patterns\"\"\"\n",
        "        quality = 0\n",
        "\n",
        "        # High quality context indicators\n",
        "        if 'tabular_context' in candidate.context_indicators:\n",
        "            quality += 0.8\n",
        "        elif 'measurement_context' in candidate.context_indicators:\n",
        "            quality += 0.6\n",
        "        elif 'graphical_context' in candidate.context_indicators:\n",
        "            quality += 0.4\n",
        "\n",
        "        # Property-specific context matching\n",
        "        prop_indicators = [ind for ind in candidate.context_indicators if ind.startswith('property_')]\n",
        "        prop_keywords = prop.learned_characteristics.get('semantic_keywords', [])\n",
        "\n",
        "        indicator_matches = sum(1 for ind in prop_indicators\n",
        "                               if any(keyword in ind for keyword in prop_keywords))\n",
        "\n",
        "        if indicator_matches > 0:\n",
        "            quality += 0.3\n",
        "\n",
        "        return quality\n",
        "\n",
        "    def _assess_value_reasonableness_learned(self, candidate: AdaptiveCandidate) -> float:\n",
        "        \"\"\"Assess value reasonableness using learned distributions\"\"\"\n",
        "        unit = candidate.unit\n",
        "        value = candidate.value\n",
        "\n",
        "        if unit not in self.learned_document_profile.get('value_ranges', {}):\n",
        "            return 0.5  # Unknown, neutral score\n",
        "\n",
        "        # Use learned value distribution for this unit\n",
        "        unit_stats = self.learned_document_profile['value_ranges'][unit]\n",
        "\n",
        "        observed_min = unit_stats['min_observed']\n",
        "        observed_max = unit_stats['max_observed']\n",
        "        median_value = unit_stats['median']\n",
        "\n",
        "        # Score based on how well value fits learned distribution\n",
        "        if observed_min <= value <= observed_max:\n",
        "            # Within observed range\n",
        "            if abs(value - median_value) <= unit_stats.get('std', 0) * 2:\n",
        "                return 1.0  # Close to typical values\n",
        "            else:\n",
        "                return 0.7  # Within range but not typical\n",
        "        else:\n",
        "            # Outside observed range - check if reasonable extension\n",
        "            range_size = observed_max - observed_min\n",
        "            if (value < observed_min and (observed_min - value) <= range_size * 0.5) or \\\n",
        "               (value > observed_max and (value - observed_max) <= range_size * 0.5):\n",
        "                return 0.4  # Reasonable extension\n",
        "            else:\n",
        "                return 0.1  # Likely unreasonable\n",
        "\n",
        "    def _assess_value_consistency(self, candidate: AdaptiveCandidate) -> float:\n",
        "        \"\"\"Assess consistency with surrounding values\"\"\"\n",
        "        if not candidate.surrounding_values:\n",
        "            return 0.5  # Neutral if no surrounding context\n",
        "\n",
        "        # Check if value fits well with surrounding values\n",
        "        surrounding_same_unit = [v for v, u in candidate.surrounding_values if u == candidate.unit]\n",
        "\n",
        "        if surrounding_same_unit:\n",
        "            # Value should be reasonably consistent with nearby values of same unit\n",
        "            nearby_values = np.array(surrounding_same_unit)\n",
        "            if len(nearby_values) > 1:\n",
        "                mean_nearby = np.mean(nearby_values)\n",
        "                std_nearby = np.std(nearby_values)\n",
        "\n",
        "                # Score based on how well value fits with nearby values\n",
        "                if abs(candidate.value - mean_nearby) <= std_nearby * 2:\n",
        "                    return 1.0  # Consistent with nearby values\n",
        "                else:\n",
        "                    return 0.3  # Inconsistent\n",
        "\n",
        "        return 0.5  # Neutral\n",
        "\n",
        "    def _calculate_adaptive_threshold(self, prop: LearnedProperty, candidates: List[AdaptiveCandidate]) -> float:\n",
        "        \"\"\"Calculate adaptive threshold based on data quality\"\"\"\n",
        "        # Learn threshold from available candidate quality\n",
        "        candidate_scores = []\n",
        "\n",
        "        for candidate in candidates:\n",
        "            if not candidate.is_assigned:\n",
        "                score = len(candidate.data_quality_indicators) * 10 + len(candidate.context_indicators) * 5\n",
        "                candidate_scores.append(score)\n",
        "\n",
        "        if not candidate_scores:\n",
        "            return 50  # Default\n",
        "\n",
        "        # Adaptive threshold: use 75th percentile of candidate quality\n",
        "        threshold = np.percentile(candidate_scores, 75) if len(candidate_scores) > 4 else np.mean(candidate_scores)\n",
        "\n",
        "        # Ensure reasonable bounds\n",
        "        return max(40, min(80, threshold))\n",
        "\n",
        "    def _validate_with_learned_constraints(self, candidate: AdaptiveCandidate, prop: LearnedProperty) -> bool:\n",
        "        \"\"\"Validate using constraints learned from document\"\"\"\n",
        "        unit = candidate.unit\n",
        "        value = candidate.value\n",
        "\n",
        "        # Use learned value ranges for validation\n",
        "        if unit in self.learned_document_profile.get('value_ranges', {}):\n",
        "            unit_stats = self.learned_document_profile['value_ranges'][unit]\n",
        "\n",
        "            # Allow values within reasonable extension of observed range\n",
        "            observed_min = unit_stats['min_observed']\n",
        "            observed_max = unit_stats['max_observed']\n",
        "            range_size = observed_max - observed_min\n",
        "\n",
        "            # Expanded range (50% extension of observed range)\n",
        "            extended_min = observed_min - range_size * 0.5\n",
        "            extended_max = observed_max + range_size * 0.5\n",
        "\n",
        "            if not (extended_min <= value <= extended_max):\n",
        "                return False\n",
        "\n",
        "        # Semantic consistency validation\n",
        "        measurement_type = prop.learned_characteristics.get('likely_measurement_type', 'unknown')\n",
        "\n",
        "        # Learn from document what units typically go with what measurement types\n",
        "        if unit in self.learned_document_profile.get('unit_characteristics', {}):\n",
        "            unit_profile = self.learned_document_profile['unit_characteristics'][unit]\n",
        "            associated_types = unit_profile.get('appears_with_properties', [])\n",
        "\n",
        "            type_mapping = {\n",
        "                'stiffness': 'modulus_type',\n",
        "                'stress': 'strength_type',\n",
        "                'deformation': 'strain_type'\n",
        "            }\n",
        "\n",
        "            expected_type = type_mapping.get(measurement_type, 'unknown')\n",
        "\n",
        "            # If we learned this unit goes with different property types, be cautious\n",
        "            if associated_types and expected_type not in associated_types:\n",
        "                return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _assign_adaptive_confidence(self, score: float, candidate: AdaptiveCandidate, threshold: float) -> str:\n",
        "        \"\"\"Assign confidence using adaptive criteria\"\"\"\n",
        "        # Adaptive confidence based on how much score exceeds threshold\n",
        "        score_margin = score - threshold\n",
        "\n",
        "        if (score >= 90 and\n",
        "            'high_quality_source' in candidate.data_quality_indicators and\n",
        "            'tabular_context' in candidate.context_indicators):\n",
        "            return 'high'\n",
        "        elif score_margin >= 15 and 'medium_quality_source' in candidate.data_quality_indicators:\n",
        "            return 'medium'\n",
        "        elif score_margin >= 5:\n",
        "            return 'low'\n",
        "        else:\n",
        "            return 'very_low'\n",
        "\n",
        "# Initialize adaptive extractor\n",
        "print(\"Initializing Adaptive Property Extractor (Zero Hardcoding)...\")\n",
        "extractor = AdaptivePropertyExtractor()\n",
        "\n",
        "# Global state\n",
        "pdf_text_global = \"\"\n",
        "images_global = {}\n",
        "properties_global = []\n",
        "results_global = {}\n",
        "learning_profile_global = {}\n",
        "\n",
        "def process_files_adaptively(pdf_file, excel_file):\n",
        "    \"\"\"Process files with complete adaptation\"\"\"\n",
        "    global pdf_text_global, images_global, properties_global, learning_profile_global\n",
        "\n",
        "    if not pdf_file or not excel_file:\n",
        "        return \"Upload both files\", \"Missing files\"\n",
        "\n",
        "    try:\n",
        "        print(\"\\nADAPTIVE DOCUMENT LEARNING\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Learn from PDF\n",
        "        pdf_text, images, doc_profile = extractor.extract_pdf_and_learn(pdf_file)\n",
        "\n",
        "        if pdf_text.startswith(\"Error\"):\n",
        "            return pdf_text, \"PDF learning failed\"\n",
        "\n",
        "        pdf_text_global = pdf_text\n",
        "        images_global = images\n",
        "        learning_profile_global = doc_profile\n",
        "\n",
        "        # Learn from Excel\n",
        "        properties = extractor.read_excel_and_learn(excel_file)\n",
        "\n",
        "        if not properties:\n",
        "            return pdf_text[:1000], \"Excel learning failed\"\n",
        "\n",
        "        properties_global = properties\n",
        "\n",
        "        # Adaptive summary\n",
        "        units_learned = len(extractor.learned_document_profile.get('unit_characteristics', {}))\n",
        "        value_ranges_learned = len(extractor.learned_document_profile.get('value_ranges', {}))\n",
        "\n",
        "        summary = f\"ADAPTIVE LEARNING COMPLETE:\\n\"\n",
        "        summary += f\"PDF: {len(pdf_text)} characters, {len(images)} images\\n\"\n",
        "        summary += f\"Document units learned: {units_learned}\\n\"\n",
        "        summary += f\"Value distributions learned: {value_ranges_learned}\\n\"\n",
        "        summary += f\"Context types identified: {len(doc_profile.get('context_types', {}))}\\n\"\n",
        "        summary += f\"Excel properties learned: {len(properties)}\\n\"\n",
        "        summary += f\"Categories identified: {len(set([p.category for p in properties]))}\\n\"\n",
        "        summary += f\"Semantic profiles created: {len([p for p in properties if p.semantic_profile])}\\n\"\n",
        "        summary += f\"Learning approach: ZERO HARDCODING\"\n",
        "\n",
        "        preview = pdf_text[:1200] + \"...\" if len(pdf_text) > 1200 else pdf_text\n",
        "\n",
        "        return preview, summary\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Learning error: {str(e)}\", \"Adaptive learning failed\"\n",
        "\n",
        "def extract_with_adaptive_learning():\n",
        "    \"\"\"Extract using pure adaptive learning\"\"\"\n",
        "    global pdf_text_global, images_global, properties_global, results_global, learning_profile_global\n",
        "\n",
        "    if not pdf_text_global or not properties_global:\n",
        "        return pd.DataFrame([{\"Error\": \"Complete adaptive learning first\"}]), \"No learned data\", \"\"\n",
        "\n",
        "    try:\n",
        "        print(\"\\nADAPTIVE EXTRACTION (ZERO HARDCODING)\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Discover candidates using learned patterns\n",
        "        candidates = extractor.discover_all_candidates_adaptively(pdf_text_global)\n",
        "\n",
        "        # Match using adaptive learning\n",
        "        adaptive_results = extractor.match_adaptively_with_learning(properties_global, candidates)\n",
        "\n",
        "        results_global = adaptive_results\n",
        "\n",
        "        # Create adaptive results table\n",
        "        table_rows = []\n",
        "        for prop in properties_global:\n",
        "            data = adaptive_results.get(prop.name, {})\n",
        "            table_rows.append({\n",
        "                \"Property\": prop.name,\n",
        "                \"Value\": data.get('value', 'N/A'),\n",
        "                \"Unit\": data.get('unit', 'N/A'),\n",
        "                \"Source\": data.get('source', 'N/A'),\n",
        "                \"Confidence\": data.get('confidence', 'none'),\n",
        "                \"Adaptive Score\": f\"{data.get('adaptive_score', 0):.1f}\",\n",
        "                \"Learned Threshold\": f\"{data.get('adaptive_threshold', 0):.1f}\",\n",
        "                \"Validation\": data.get('validation_status', 'not_processed'),\n",
        "                \"Learning Basis\": data.get('learned_basis', 'N/A')\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(table_rows)\n",
        "\n",
        "        # Adaptive statistics\n",
        "        found_values = len([r for r in adaptive_results.values() if r.get('value') != 'N/A'])\n",
        "        adaptive_matches = len([r for r in adaptive_results.values() if r.get('validation_status') == 'adaptive_match'])\n",
        "        used_candidates = len([c for c in candidates if c.is_assigned])\n",
        "\n",
        "        # Calculate adaptive success metrics\n",
        "        if candidates:\n",
        "            candidate_utilization = (used_candidates / len(candidates)) * 100\n",
        "        else:\n",
        "            candidate_utilization = 0\n",
        "\n",
        "        # Status with learning insights\n",
        "        status = f\"ADAPTIVE EXTRACTION RESULTS (ZERO HARDCODING):\\n\"\n",
        "        status += f\"Learning approach: Document-driven adaptation\\n\"\n",
        "        status += f\"Total candidates discovered: {len(candidates)}\\n\"\n",
        "        status += f\"Candidates used adaptively: {used_candidates}\\n\"\n",
        "        status += f\"Candidate utilization rate: {candidate_utilization:.1f}%\\n\"\n",
        "        status += f\"Properties processed: {len(properties_global)}\\n\"\n",
        "        status += f\"Adaptive matches found: {adaptive_matches}\\n\"\n",
        "        status += f\"Success rate: {(found_values/len(properties_global)*100):.1f}%\\n\"\n",
        "        status += f\"Learned units: {list(extractor.learned_document_profile.get('unit_characteristics', {}).keys())}\\n\"\n",
        "        status += f\"Adaptive thresholds used: Variable (learned per property)\\n\"\n",
        "        status += f\"Hardcoded values: ZERO\"\n",
        "\n",
        "        # Adaptive JSON output\n",
        "        adaptive_json = {\n",
        "            \"adaptive_extraction_metadata\": {\n",
        "                \"version\": \"adaptive_zero_hardcode_v1\",\n",
        "                \"learning_approach\": \"document_driven_adaptation\",\n",
        "                \"hardcoded_values\": 0,\n",
        "                \"hardcoded_thresholds\": 0,\n",
        "                \"hardcoded_ranges\": 0,\n",
        "                \"adaptive_features\": [\n",
        "                    \"learned_unit_compatibility\",\n",
        "                    \"adaptive_scoring_thresholds\",\n",
        "                    \"document_pattern_learning\",\n",
        "                    \"semantic_profile_matching\",\n",
        "                    \"value_distribution_learning\"\n",
        "                ],\n",
        "                \"extraction_stats\": {\n",
        "                    \"total_properties\": len(properties_global),\n",
        "                    \"adaptive_matches\": adaptive_matches,\n",
        "                    \"candidate_utilization\": f\"{candidate_utilization:.1f}%\",\n",
        "                    \"success_rate\": f\"{(found_values/len(properties_global)*100):.1f}%\"\n",
        "                },\n",
        "                \"learned_document_profile\": extractor.learned_document_profile,\n",
        "                \"timestamp\": datetime.now().isoformat()\n",
        "            },\n",
        "            \"adaptive_results\": adaptive_results\n",
        "        }\n",
        "\n",
        "        json_output = json.dumps(adaptive_json, indent=2, default=str)\n",
        "\n",
        "        return df, status, json_output\n",
        "\n",
        "    except Exception as e:\n",
        "        error_df = pd.DataFrame([{\"Error\": f\"Adaptive extraction failed: {str(e)}\"}])\n",
        "        return error_df, f\"Adaptive error: {str(e)}\", \"\"\n",
        "\n",
        "def download_adaptive_results():\n",
        "    \"\"\"Download adaptive results\"\"\"\n",
        "    global results_global, properties_global\n",
        "\n",
        "    if not results_global:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"adaptive_zero_hardcode_results_{timestamp}.json\"\n",
        "\n",
        "        # Comprehensive adaptive output\n",
        "        adaptive_output = {\n",
        "            \"adaptive_extraction_info\": {\n",
        "                \"version\": \"adaptive_zero_hardcode_final\",\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"learning_methodology\": \"document_driven_adaptation\",\n",
        "                \"hardcoded_components\": [],\n",
        "                \"adaptive_components\": [\n",
        "                    \"unit_compatibility_learning\",\n",
        "                    \"threshold_adaptation\",\n",
        "                    \"semantic_profile_generation\",\n",
        "                    \"value_distribution_analysis\",\n",
        "                    \"context_quality_learning\"\n",
        "                ]\n",
        "            },\n",
        "            \"learned_document_insights\": extractor.learned_document_profile,\n",
        "            \"extraction_results\": results_global,\n",
        "            \"learning_summary\": {\n",
        "                \"total_properties_analyzed\": len(properties_global),\n",
        "                \"extraction_methodology\": \"pure_adaptive_learning\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(adaptive_output, f, indent=2, default=str)\n",
        "\n",
        "        return filename\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Adaptive Gradio Interface\n",
        "with gr.Blocks(title=\"Adaptive Property Extractor - Zero Hardcoding\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # Adaptive Property Extractor - Zero Hardcoded Values\n",
        "    **Learns everything from your documents • Adaptive thresholds • Pure data-driven approach**\n",
        "\n",
        "    This system learns unit compatibility, value ranges, and validation criteria entirely from your documents.\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Tab(\"Adaptive Learning\"):\n",
        "        gr.Markdown(\"### Document Learning Phase\")\n",
        "\n",
        "        with gr.Row():\n",
        "            pdf_input = gr.File(label=\"PDF Document (Learning Source)\", file_types=[\".pdf\"])\n",
        "            excel_input = gr.File(label=\"Excel Template (Property Definitions)\", file_types=[\".xlsx\", \".xls\"])\n",
        "\n",
        "        learn_btn = gr.Button(\"Learn from Documents\", variant=\"primary\")\n",
        "\n",
        "        with gr.Row():\n",
        "            pdf_preview = gr.Textbox(label=\"PDF Content Preview\", lines=10)\n",
        "            learning_summary = gr.Textbox(label=\"Adaptive Learning Summary\", lines=10)\n",
        "\n",
        "    with gr.Tab(\"Adaptive Extraction\"):\n",
        "        gr.Markdown(\"### Zero-Hardcoding Extraction\")\n",
        "\n",
        "        extract_btn = gr.Button(\"Extract with Adaptive Learning\", variant=\"secondary\", size=\"lg\")\n",
        "\n",
        "        results_table = gr.Dataframe(\n",
        "            label=\"Adaptive Extraction Results\",\n",
        "            headers=[\"Property\", \"Value\", \"Unit\", \"Source\", \"Confidence\",\n",
        "                    \"Adaptive Score\", \"Learned Threshold\", \"Validation\", \"Learning Basis\"]\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            adaptive_status = gr.Textbox(label=\"Adaptive Extraction Status\", lines=10)\n",
        "            adaptive_json = gr.Code(label=\"Adaptive Results JSON\", language=\"json\", lines=12)\n",
        "\n",
        "    with gr.Tab(\"Learning Insights\"):\n",
        "        download_btn = gr.Button(\"Download Adaptive Results\")\n",
        "        file_output = gr.File(label=\"Adaptive Results File\")\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        **Zero Hardcoding Features:**\n",
        "        - Unit compatibility learned from document co-occurrence patterns\n",
        "        - Thresholds calculated adaptively from candidate quality distribution\n",
        "        - Value ranges learned from document value distributions\n",
        "        - Property-unit associations discovered from semantic context\n",
        "        - Validation criteria derived from learned document patterns\n",
        "        - No predefined ranges, thresholds, or unit requirements\n",
        "        \"\"\")\n",
        "\n",
        "    # Connect adaptive functions\n",
        "    learn_btn.click(\n",
        "        process_files_adaptively,\n",
        "        inputs=[pdf_input, excel_input],\n",
        "        outputs=[pdf_preview, learning_summary]\n",
        "    )\n",
        "\n",
        "    extract_btn.click(\n",
        "        extract_with_adaptive_learning,\n",
        "        outputs=[results_table, adaptive_status, adaptive_json]\n",
        "    )\n",
        "\n",
        "    download_btn.click(\n",
        "        download_adaptive_results,\n",
        "        outputs=[file_output]\n",
        "    )\n",
        "\n",
        "print(\"Adaptive Property Extractor (Zero Hardcoding) ready!\")\n",
        "print(\"Features:\")\n",
        "print(\"- Learns unit compatibility from document patterns\")\n",
        "print(\"- Adapts thresholds based on candidate quality\")\n",
        "print(\"- Discovers value ranges from document data\")\n",
        "print(\"- Zero predefined assumptions\")\n",
        "\n",
        "demo.launch(debug=True, share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "y98LnKfhXfYO",
        "outputId": "a9068848-33a0-44da-db3b-68db6be7606d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Practical Property Extractor ready!\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://7673e8d676ac33f3c0.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7673e8d676ac33f3c0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://7673e8d676ac33f3c0.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Practical Adaptive Property Extractor\n",
        "# Essential engineering knowledge + Document-specific learning\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Quick setup for Colab\n",
        "def quick_setup():\n",
        "    try:\n",
        "        import fitz\n",
        "        import gradio as gr\n",
        "    except ImportError:\n",
        "        packages = [\"PyMuPDF==1.23.26\", \"gradio\", \"requests\", \"pandas\", \"numpy\", \"Pillow\"]\n",
        "        for package in packages:\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], capture_output=True)\n",
        "\n",
        "quick_setup()\n",
        "\n",
        "import re\n",
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, List, Any, Tuple, Optional\n",
        "from collections import defaultdict, Counter\n",
        "from datetime import datetime\n",
        "import base64\n",
        "import io\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import fitz\n",
        "\n",
        "# API Configuration\n",
        "GEMINI_API_KEY = \"AIzaSyCFzlJFsIq6PYLuHSPqLYvg0clx-CPpSD0\"\n",
        "GEMINI_ENDPOINT = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent\"\n",
        "\n",
        "class PracticalExtractor:\n",
        "    def __init__(self):\n",
        "        # Essential: Valid engineering units (prevents \"NPL\", \"Modelling\" extraction)\n",
        "        self.engineering_units = self._get_engineering_units()\n",
        "\n",
        "        # Adaptive: Learned from document\n",
        "        self.document_patterns = {}\n",
        "        self.value_contexts = defaultdict(list)\n",
        "        self.used_extractions = set()\n",
        "\n",
        "    def _get_engineering_units(self):\n",
        "        \"\"\"Essential engineering units - prevents garbage extraction\"\"\"\n",
        "        return {\n",
        "            'MPa': re.compile(r'(\\d+\\.?\\d*)\\s*MPa(?!\\w)', re.I),\n",
        "            'GPa': re.compile(r'(\\d+\\.?\\d*)\\s*GPa(?!\\w)', re.I),\n",
        "            'Pa': re.compile(r'(\\d+\\.?\\d*)\\s*Pa(?![a-zA-Z])', re.I),\n",
        "            '%': re.compile(r'(\\d+\\.?\\d*)\\s*%(?!\\w)'),\n",
        "            's⁻¹': re.compile(r'(\\d+\\.?\\d*(?:[×x]10[-−]?\\d+)?)\\s*s[-−]?1(?!\\w)', re.I),\n",
        "            '°C': re.compile(r'(\\d+\\.?\\d*)\\s*°C(?!\\w)'),\n",
        "            'J/m²': re.compile(r'(\\d+\\.?\\d*)\\s*J/m[²2](?!\\w)', re.I),\n",
        "            'g/cm³': re.compile(r'(\\d+\\.?\\d*)\\s*g/cm[³3](?!\\w)', re.I),\n",
        "            'dimensionless': re.compile(r'(?<!\\d)(\\d\\.\\d{2,4})(?!\\s*[A-Za-z%])')\n",
        "        }\n",
        "\n",
        "    def analyze_document_content(self, pdf_file):\n",
        "        \"\"\"Analyze what's actually in the document\"\"\"\n",
        "        try:\n",
        "            doc = fitz.open(pdf_file.name)\n",
        "            full_text = \"\"\n",
        "\n",
        "            for page in doc:\n",
        "                full_text += page.get_text()\n",
        "\n",
        "            doc.close()\n",
        "\n",
        "            # Learn what's actually available\n",
        "            available_data = self._discover_available_data(full_text)\n",
        "            document_type = self._classify_document_type(full_text)\n",
        "\n",
        "            return {\n",
        "                'text': full_text,\n",
        "                'available_data': available_data,\n",
        "                'document_type': document_type,\n",
        "                'data_summary': self._create_data_summary(available_data)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {'error': f\"Document analysis failed: {e}\"}\n",
        "\n",
        "    def _discover_available_data(self, text: str) -> List[Dict]:\n",
        "        \"\"\"Discover what numerical data is actually available\"\"\"\n",
        "        available_data = []\n",
        "        lines = text.split('\\n')\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            line_clean = line.strip()\n",
        "            if not line_clean:\n",
        "                continue\n",
        "\n",
        "            # Skip structural elements\n",
        "            if self._is_document_structure(line_clean):\n",
        "                continue\n",
        "\n",
        "            # Extract values with engineering units\n",
        "            for unit, pattern in self.engineering_units.items():\n",
        "                for match in pattern.finditer(line_clean):\n",
        "                    try:\n",
        "                        value_str = match.group(1)\n",
        "                        value = float(value_str.replace('×', 'e').replace('−', '-'))\n",
        "\n",
        "                        if self._is_meaningful_value(value, unit, line_clean):\n",
        "                            context = self._get_context(lines, i)\n",
        "                            data_type = self._identify_data_type(context)\n",
        "\n",
        "                            available_data.append({\n",
        "                                'value': value,\n",
        "                                'unit': unit,\n",
        "                                'context': line_clean,\n",
        "                                'full_context': context,\n",
        "                                'data_type': data_type,\n",
        "                                'line_number': i,\n",
        "                                'quality': self._assess_quality(context, unit)\n",
        "                            })\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "        return available_data\n",
        "\n",
        "    def _is_document_structure(self, line: str) -> bool:\n",
        "        \"\"\"Filter document structure elements\"\"\"\n",
        "        filters = [\n",
        "            r'^\\d+\\.\\s*[A-Z]',  # Section headers\n",
        "            r'©.*\\d{4}',  # Copyright\n",
        "            r'published by',  # Publication info\n",
        "            r'^\\s*\\[\\d+\\]',  # References\n",
        "            r'^(Abstract|Introduction|Conclusion|References)$'  # Section titles\n",
        "        ]\n",
        "\n",
        "        return any(re.search(pattern, line, re.I) for pattern in filters)\n",
        "\n",
        "    def _is_meaningful_value(self, value: float, unit: str, context: str) -> bool:\n",
        "        \"\"\"Check if value is meaningful (not composition)\"\"\"\n",
        "        if value <= 0 or value > 1e8:\n",
        "            return False\n",
        "\n",
        "        # Filter out material compositions\n",
        "        context_lower = context.lower()\n",
        "        if any(phrase in context_lower for phrase in [\n",
        "            'containing', 'composed of', 'copolymer', 'wt%', 'vol%'\n",
        "        ]):\n",
        "            return False\n",
        "\n",
        "        # Basic reasonableness\n",
        "        if unit == 'dimensionless' and value > 100:\n",
        "            return False\n",
        "        elif unit == '%' and 'containing' in context_lower:\n",
        "            return False  # Material composition\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _get_context(self, lines: List[str], center: int) -> str:\n",
        "        \"\"\"Get context around line\"\"\"\n",
        "        start = max(0, center - 2)\n",
        "        end = min(len(lines), center + 3)\n",
        "        return ' '.join([line.strip() for line in lines[start:end] if line.strip()])\n",
        "\n",
        "    def _identify_data_type(self, context: str) -> str:\n",
        "        \"\"\"Identify type of data\"\"\"\n",
        "        context_lower = context.lower()\n",
        "\n",
        "        if any(word in context_lower for word in ['table', 'data', 'values']):\n",
        "            return 'tabular_data'\n",
        "        elif any(word in context_lower for word in ['measured', 'tested', 'experimental']):\n",
        "            return 'experimental_data'\n",
        "        elif any(word in context_lower for word in ['parameter', 'coefficient']):\n",
        "            return 'model_parameter'\n",
        "        elif any(word in context_lower for word in ['condition', 'speed', 'rate']):\n",
        "            return 'test_condition'\n",
        "        else:\n",
        "            return 'general_text'\n",
        "\n",
        "    def _assess_quality(self, context: str, unit: str) -> str:\n",
        "        \"\"\"Assess data quality\"\"\"\n",
        "        context_lower = context.lower()\n",
        "\n",
        "        if any(word in context_lower for word in ['table', 'data']):\n",
        "            return 'high'\n",
        "        elif any(word in context_lower for word in ['measured', 'result']):\n",
        "            return 'medium'\n",
        "        else:\n",
        "            return 'low'\n",
        "\n",
        "    def _classify_document_type(self, text: str) -> str:\n",
        "        \"\"\"Classify document type\"\"\"\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        if any(word in text_lower for word in ['model', 'equation', 'analysis', 'methodology']):\n",
        "            return 'research_paper'\n",
        "        elif any(word in text_lower for word in ['specification', 'datasheet', 'standard']):\n",
        "            return 'technical_specification'\n",
        "        else:\n",
        "            return 'technical_document'\n",
        "\n",
        "    def _create_data_summary(self, available_data: List[Dict]) -> Dict:\n",
        "        \"\"\"Create summary of available data\"\"\"\n",
        "        summary = {\n",
        "            'total_data_points': len(available_data),\n",
        "            'units_found': list(set([d['unit'] for d in available_data])),\n",
        "            'data_types': dict(Counter([d['data_type'] for d in available_data])),\n",
        "            'quality_distribution': dict(Counter([d['quality'] for d in available_data]))\n",
        "        }\n",
        "        return summary\n",
        "\n",
        "    def match_data_to_properties(self, available_data: List[Dict], excel_properties: List[str]) -> Dict:\n",
        "        \"\"\"Match available data to Excel properties realistically\"\"\"\n",
        "        results = {}\n",
        "        used_data = set()\n",
        "\n",
        "        # Sort properties by likelihood of finding data\n",
        "        prioritized_props = self._prioritize_properties(excel_properties)\n",
        "\n",
        "        for prop_name in prioritized_props:\n",
        "            best_match = None\n",
        "            best_score = 0\n",
        "\n",
        "            for data in available_data:\n",
        "                data_key = (data['value'], data['unit'])\n",
        "                if data_key in used_data:\n",
        "                    continue\n",
        "\n",
        "                # Score match between data and property\n",
        "                score = self._score_data_property_match(data, prop_name)\n",
        "\n",
        "                if score > best_score and score >= 50:  # Reasonable threshold\n",
        "                    best_match = data\n",
        "                    best_score = score\n",
        "\n",
        "            if best_match and best_score >= 50:\n",
        "                used_data.add((best_match['value'], best_match['unit']))\n",
        "\n",
        "                results[prop_name] = {\n",
        "                    'value': f\"{best_match['value']} {best_match['unit']}\",\n",
        "                    'unit': best_match['unit'],\n",
        "                    'source': f\"Page data - {best_match['data_type']}\",\n",
        "                    'confidence': best_match['quality'],\n",
        "                    'score': best_score,\n",
        "                    'context': best_match['context'][:100],\n",
        "                    'data_type': best_match['data_type']\n",
        "                }\n",
        "            else:\n",
        "                results[prop_name] = {\n",
        "                    'value': 'N/A',\n",
        "                    'unit': 'N/A',\n",
        "                    'source': 'Not found in document',\n",
        "                    'confidence': 'none',\n",
        "                    'score': best_score,\n",
        "                    'reason': 'Data may not exist in this document type'\n",
        "                }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _prioritize_properties(self, properties: List[str]) -> List[str]:\n",
        "        \"\"\"Prioritize properties by likelihood of finding in research papers\"\"\"\n",
        "        def get_priority(prop: str) -> int:\n",
        "            prop_lower = prop.lower()\n",
        "            priority = 0\n",
        "\n",
        "            # Higher priority for properties likely in research papers\n",
        "            if any(word in prop_lower for word in ['stress', 'yield', 'modulus']):\n",
        "                priority += 20\n",
        "            if any(word in prop_lower for word in ['tensile', 'test', 'condition']):\n",
        "                priority += 15\n",
        "            if any(word in prop_lower for word in ['temperature', 'rate']):\n",
        "                priority += 10\n",
        "\n",
        "            return priority\n",
        "\n",
        "        return sorted(properties, key=get_priority, reverse=True)\n",
        "\n",
        "    def _score_data_property_match(self, data: Dict, property_name: str) -> float:\n",
        "        \"\"\"Score how well data matches property\"\"\"\n",
        "        score = 0\n",
        "        prop_lower = property_name.lower()\n",
        "        context_lower = data['context'].lower()\n",
        "\n",
        "        # Unit appropriateness\n",
        "        if self._is_unit_appropriate(data['unit'], prop_lower):\n",
        "            score += 30\n",
        "\n",
        "        # Keyword matching\n",
        "        prop_keywords = [word for word in prop_lower.split() if len(word) > 3]\n",
        "        keyword_matches = sum(1 for word in prop_keywords if word in context_lower)\n",
        "        score += keyword_matches * 15\n",
        "\n",
        "        # Data type appropriateness\n",
        "        if data['data_type'] in ['tabular_data', 'experimental_data']:\n",
        "            score += 20\n",
        "        elif data['data_type'] == 'model_parameter':\n",
        "            score += 15\n",
        "\n",
        "        # Quality bonus\n",
        "        if data['quality'] == 'high':\n",
        "            score += 15\n",
        "        elif data['quality'] == 'medium':\n",
        "            score += 10\n",
        "\n",
        "        return score\n",
        "\n",
        "    def _is_unit_appropriate(self, unit: str, property_name: str) -> bool:\n",
        "        \"\"\"Check if unit is appropriate for property\"\"\"\n",
        "        if 'modulus' in property_name:\n",
        "            return unit in ['GPa', 'MPa']\n",
        "        elif any(word in property_name for word in ['strength', 'stress']):\n",
        "            return unit in ['MPa', 'GPa']\n",
        "        elif any(word in property_name for word in ['strain', 'elongation']):\n",
        "            return unit in ['%', 'dimensionless']\n",
        "        elif 'temperature' in property_name:\n",
        "            return unit in ['°C']\n",
        "        elif 'rate' in property_name:\n",
        "            return unit in ['s⁻¹']\n",
        "        elif 'energy' in property_name:\n",
        "            return unit in ['J/m²']\n",
        "\n",
        "        return True\n",
        "\n",
        "    def extract_with_ai_assistance(self, pdf_text: str, target_properties: List[str]) -> Dict:\n",
        "        \"\"\"Use AI to extract specific property data\"\"\"\n",
        "        try:\n",
        "            # Focus on properties most likely to be in document\n",
        "            realistic_props = [p for p in target_properties[:10] if self._is_realistic_for_research_paper(p)]\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "EXTRACT SPECIFIC PROPERTY DATA FROM RESEARCH DOCUMENT\n",
        "\n",
        "TARGET PROPERTIES (only extract if explicitly mentioned):\n",
        "{json.dumps(realistic_props, indent=1)}\n",
        "\n",
        "DOCUMENT EXCERPT:\n",
        "{pdf_text[:8000]}\n",
        "\n",
        "CRITICAL INSTRUCTIONS:\n",
        "1. Only extract explicit numerical values with engineering units (MPa, GPa, %, °C, s⁻¹)\n",
        "2. Distinguish material composition from properties:\n",
        "   - \"containing 8% ethylene\" = material composition (DO NOT extract as property)\n",
        "   - \"yield stress 38 MPa\" = property data (CAN extract)\n",
        "3. Prefer experimental data and measurements over model parameters\n",
        "4. Each value should be used for only one property\n",
        "\n",
        "RESPONSE FORMAT:\n",
        "{{\n",
        "  \"PropertyName\": {{\n",
        "    \"value\": \"number unit\",\n",
        "    \"unit\": \"unit_only\",\n",
        "    \"source\": \"description\",\n",
        "    \"confidence\": \"high/medium/low\",\n",
        "    \"context\": \"where_found\"\n",
        "  }}\n",
        "}}\n",
        "\n",
        "Return only confident extractions in valid JSON.\n",
        "\"\"\"\n",
        "\n",
        "            response = requests.post(\n",
        "                f\"{GEMINI_ENDPOINT}?key={GEMINI_API_KEY}\",\n",
        "                json={\n",
        "                    \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
        "                    \"generationConfig\": {\"temperature\": 0.0, \"maxOutputTokens\": 1200}\n",
        "                },\n",
        "                timeout=60\n",
        "            )\n",
        "\n",
        "            if response.ok:\n",
        "                result = response.json()\n",
        "                content = result['candidates'][0]['content']['parts'][0]['text']\n",
        "                return self._parse_json_response(content)\n",
        "\n",
        "            return {}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"AI extraction failed: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def _is_realistic_for_research_paper(self, prop: str) -> bool:\n",
        "        \"\"\"Check if property is realistic to find in research papers\"\"\"\n",
        "        prop_lower = prop.lower()\n",
        "\n",
        "        # Properties often found in research papers\n",
        "        realistic_indicators = [\n",
        "            'stress', 'strain', 'modulus', 'yield', 'temperature',\n",
        "            'rate', 'speed', 'parameter'\n",
        "        ]\n",
        "\n",
        "        return any(indicator in prop_lower for indicator in realistic_indicators)\n",
        "\n",
        "    def _parse_json_response(self, content: str) -> Dict:\n",
        "        \"\"\"Parse AI JSON response\"\"\"\n",
        "        try:\n",
        "            if '```json' in content:\n",
        "                content = content.split('```json')[1].split('```')[0]\n",
        "\n",
        "            start = content.find('{')\n",
        "            end = content.rfind('}') + 1\n",
        "\n",
        "            if start != -1 and end > start:\n",
        "                return json.loads(content[start:end])\n",
        "            return {}\n",
        "        except:\n",
        "            return {}\n",
        "\n",
        "    def read_excel_properties(self, excel_file) -> List[str]:\n",
        "        \"\"\"Read Excel properties flexibly\"\"\"\n",
        "        try:\n",
        "            df = None\n",
        "            for header in [None, 0, 1]:\n",
        "                try:\n",
        "                    df = pd.read_excel(excel_file, header=header)\n",
        "                    if len(df) >= 3:\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if df is None:\n",
        "                return []\n",
        "\n",
        "            properties = []\n",
        "            for _, row in df.iterrows():\n",
        "                for cell in row:\n",
        "                    if pd.notna(cell):\n",
        "                        prop = str(cell).strip()\n",
        "                        if (len(prop) > 3 and\n",
        "                            prop.lower() not in ['description', 'value', 'unit', 'note'] and\n",
        "                            not prop.lower().startswith('unnamed')):\n",
        "                            properties.append(prop)\n",
        "                            break\n",
        "\n",
        "            return properties[:30]  # Reasonable limit\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Excel reading error: {e}\")\n",
        "            return []\n",
        "\n",
        "# Initialize extractor\n",
        "extractor = PracticalExtractor()\n",
        "\n",
        "# Global state\n",
        "document_data = {}\n",
        "excel_properties = []\n",
        "extraction_results = {}\n",
        "\n",
        "def analyze_documents(pdf_file, excel_file):\n",
        "    \"\"\"Analyze what's actually in the documents\"\"\"\n",
        "    global document_data, excel_properties\n",
        "\n",
        "    if not pdf_file or not excel_file:\n",
        "        return \"Upload both files\", \"No files provided\"\n",
        "\n",
        "    try:\n",
        "        # Analyze PDF content\n",
        "        document_data = extractor.analyze_document_content(pdf_file)\n",
        "\n",
        "        if 'error' in document_data:\n",
        "            return document_data['error'], \"PDF analysis failed\"\n",
        "\n",
        "        # Read Excel properties\n",
        "        excel_properties = extractor.read_excel_properties(excel_file)\n",
        "\n",
        "        if not excel_properties:\n",
        "            return \"Could not read Excel properties\", \"Excel reading failed\"\n",
        "\n",
        "        # Create realistic summary\n",
        "        available_data = document_data.get('available_data', [])\n",
        "        data_summary = document_data.get('data_summary', {})\n",
        "\n",
        "        analysis_text = f\"DOCUMENT ANALYSIS:\\n\"\n",
        "        analysis_text += f\"Document Type: {document_data.get('document_type', 'unknown')}\\n\"\n",
        "        analysis_text += f\"Available Data Points: {data_summary.get('total_data_points', 0)}\\n\"\n",
        "        analysis_text += f\"Engineering Units Found: {len(data_summary.get('units_found', []))}\\n\"\n",
        "        analysis_text += f\"Data Types: {data_summary.get('data_types', {})}\\n\"\n",
        "        analysis_text += f\"Quality Distribution: {data_summary.get('quality_distribution', {})}\\n\"\n",
        "        analysis_text += f\"Excel Properties: {len(excel_properties)}\\n\"\n",
        "\n",
        "        # Show available data points\n",
        "        preview_text = \"AVAILABLE NUMERICAL DATA:\\n\"\n",
        "        for i, dp in enumerate(available_data[:15]):\n",
        "            preview_text += f\"{i+1}. {dp['value']} {dp['unit']} - {dp['data_type']}\\n\"\n",
        "            preview_text += f\"   Context: {dp['context'][:80]}...\\n\\n\"\n",
        "\n",
        "        return preview_text, analysis_text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Analysis error: {e}\", \"Document analysis failed\"\n",
        "\n",
        "def extract_realistic_matches():\n",
        "    \"\"\"Extract realistic property matches\"\"\"\n",
        "    global document_data, excel_properties, extraction_results\n",
        "\n",
        "    if not document_data or not excel_properties:\n",
        "        return pd.DataFrame([{\"Error\": \"Analyze documents first\"}]), \"No analysis data\", \"\"\n",
        "\n",
        "    try:\n",
        "        available_data = document_data.get('available_data', [])\n",
        "\n",
        "        # Match available data to Excel properties\n",
        "        rule_based_results = extractor.match_data_to_properties(available_data, excel_properties)\n",
        "\n",
        "        # Enhance with AI for missing high-priority properties\n",
        "        ai_results = extractor.extract_with_ai_assistance(document_data.get('text', ''), excel_properties)\n",
        "\n",
        "        # Merge results (AI fills gaps)\n",
        "        for prop_name, ai_data in ai_results.items():\n",
        "            if rule_based_results.get(prop_name, {}).get('value') == 'N/A':\n",
        "                rule_based_results[prop_name] = ai_data\n",
        "\n",
        "        extraction_results = rule_based_results\n",
        "\n",
        "        # Create results table\n",
        "        table_rows = []\n",
        "        for prop_name in excel_properties:\n",
        "            data = rule_based_results.get(prop_name, {})\n",
        "            table_rows.append({\n",
        "                \"Property\": prop_name,\n",
        "                \"Value\": data.get('value', 'N/A'),\n",
        "                \"Unit\": data.get('unit', 'N/A'),\n",
        "                \"Source\": data.get('source', 'N/A'),\n",
        "                \"Confidence\": data.get('confidence', 'none'),\n",
        "                \"Score\": f\"{data.get('score', 0):.1f}\",\n",
        "                \"Data Type\": data.get('data_type', 'N/A'),\n",
        "                \"Context\": data.get('context', data.get('reason', 'N/A'))\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(table_rows)\n",
        "\n",
        "        # Realistic statistics\n",
        "        found_count = len([r for r in rule_based_results.values() if r.get('value') != 'N/A'])\n",
        "        high_conf_count = len([r for r in rule_based_results.values() if r.get('confidence') == 'high'])\n",
        "\n",
        "        status_text = f\"REALISTIC EXTRACTION RESULTS:\\n\"\n",
        "        status_text += f\"Document Type: {document_data.get('document_type')}\\n\"\n",
        "        status_text += f\"Available Data Points: {len(available_data)}\\n\"\n",
        "        status_text += f\"Excel Properties Requested: {len(excel_properties)}\\n\"\n",
        "        status_text += f\"Successful Matches: {found_count}\\n\"\n",
        "        status_text += f\"High Confidence Matches: {high_conf_count}\\n\"\n",
        "        status_text += f\"Success Rate: {(found_count/len(excel_properties)*100):.1f}%\\n\"\n",
        "        status_text += f\"\\nREALITY CHECK: Research papers typically contain only a subset of standard material properties.\\n\"\n",
        "        status_text += f\"Many N/A results are expected and correct for this document type.\"\n",
        "\n",
        "        # JSON output\n",
        "        json_data = {\n",
        "            \"extraction_metadata\": {\n",
        "                \"approach\": \"realistic_document_driven_extraction\",\n",
        "                \"document_type\": document_data.get('document_type'),\n",
        "                \"total_properties\": len(excel_properties),\n",
        "                \"found_values\": found_count,\n",
        "                \"success_rate\": f\"{(found_count/len(excel_properties)*100):.1f}%\",\n",
        "                \"timestamp\": datetime.now().isoformat()\n",
        "            },\n",
        "            \"available_document_data\": available_data[:10],\n",
        "            \"extraction_results\": rule_based_results\n",
        "        }\n",
        "\n",
        "        json_output = json.dumps(json_data, indent=2)\n",
        "\n",
        "        return df, status_text, json_output\n",
        "\n",
        "    except Exception as e:\n",
        "        error_df = pd.DataFrame([{\"Error\": f\"Extraction failed: {str(e)}\"}])\n",
        "        return error_df, f\"Error: {str(e)}\", \"\"\n",
        "\n",
        "def download_realistic_results():\n",
        "    \"\"\"Download results\"\"\"\n",
        "    global extraction_results, excel_properties\n",
        "\n",
        "    if not extraction_results:\n",
        "        return None\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"realistic_extraction_{timestamp}.json\"\n",
        "\n",
        "    output = {\n",
        "        \"realistic_extraction\": {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"approach\": \"document_content_driven\",\n",
        "            \"methodology\": \"extract_available_not_expected\"\n",
        "        },\n",
        "        \"results\": extraction_results\n",
        "    }\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(output, f, indent=2)\n",
        "\n",
        "    return filename\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks(title=\"Practical Property Extractor\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # Practical Property Extractor\n",
        "    **Extracts what's actually available in your documents**\n",
        "\n",
        "    This system analyzes your document type and extracts available data realistically.\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Tab(\"Document Analysis\"):\n",
        "        with gr.Row():\n",
        "            pdf_input = gr.File(label=\"PDF Document\", file_types=[\".pdf\"])\n",
        "            excel_input = gr.File(label=\"Excel Properties\", file_types=[\".xlsx\", \".xls\"])\n",
        "\n",
        "        analyze_btn = gr.Button(\"Analyze Available Data\", variant=\"primary\")\n",
        "\n",
        "        with gr.Row():\n",
        "            available_preview = gr.Textbox(label=\"Available Data Points\", lines=15)\n",
        "            analysis_summary = gr.Textbox(label=\"Document Analysis\", lines=15)\n",
        "\n",
        "    with gr.Tab(\"Realistic Extraction\"):\n",
        "        extract_btn = gr.Button(\"Extract Available Properties\", variant=\"secondary\", size=\"lg\")\n",
        "\n",
        "        results_table = gr.Dataframe(\n",
        "            label=\"Realistic Results\",\n",
        "            headers=[\"Property\", \"Value\", \"Unit\", \"Source\", \"Confidence\", \"Score\", \"Data Type\", \"Context\"]\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            status_output = gr.Textbox(label=\"Realistic Status\", lines=12)\n",
        "            json_output = gr.Code(label=\"Results JSON\", language=\"json\", lines=10)\n",
        "\n",
        "    with gr.Tab(\"Download\"):\n",
        "        download_btn = gr.Button(\"Download Results\")\n",
        "        file_output = gr.File(label=\"Results File\")\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        **Realistic Approach:**\n",
        "        - Analyzes document type and content realistically\n",
        "        - Extracts data that actually exists\n",
        "        - Honest about document limitations\n",
        "        - Filters out material composition descriptions\n",
        "        - Focuses on experimental data and model parameters\n",
        "        - Provides realistic success expectations\n",
        "        \"\"\")\n",
        "\n",
        "    # Event handlers\n",
        "    analyze_btn.click(\n",
        "        analyze_documents,\n",
        "        inputs=[pdf_input, excel_input],\n",
        "        outputs=[available_preview, analysis_summary]\n",
        "    )\n",
        "\n",
        "    extract_btn.click(\n",
        "        extract_realistic_matches,\n",
        "        outputs=[results_table, status_output, json_output]\n",
        "    )\n",
        "\n",
        "    download_btn.click(\n",
        "        download_realistic_results,\n",
        "        outputs=[file_output]\n",
        "    )\n",
        "\n",
        "print(\"Practical Property Extractor ready!\")\n",
        "demo.launch(debug=True, share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djMAUjFzZoX4"
      },
      "outputs": [],
      "source": [
        "# Practical Adaptive Property Extractor\n",
        "# Essential engineering knowledge + Document-specific learning\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Quick setup for Colab\n",
        "def quick_setup():\n",
        "    try:\n",
        "        import fitz\n",
        "        import gradio as gr\n",
        "    except ImportError:\n",
        "        packages = [\"PyMuPDF==1.23.26\", \"gradio\", \"requests\", \"pandas\", \"numpy\", \"Pillow\"]\n",
        "        for package in packages:\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], capture_output=True)\n",
        "\n",
        "quick_setup()\n",
        "\n",
        "import re\n",
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, List, Any, Tuple, Optional\n",
        "from collections import defaultdict, Counter\n",
        "from datetime import datetime\n",
        "import base64\n",
        "import io\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import fitz\n",
        "\n",
        "# API Configuration\n",
        "GEMINI_API_KEY = \"AIzaSyCFzlJFsIq6PYLuHSPqLYvg0clx-CPpSD0\"\n",
        "GEMINI_ENDPOINT = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent\"\n",
        "\n",
        "class PracticalExtractor:\n",
        "    def __init__(self):\n",
        "        # Essential: Valid engineering units (prevents \"NPL\", \"Modelling\" extraction)\n",
        "        self.engineering_units = self._get_engineering_units()\n",
        "\n",
        "        # Adaptive: Learned from document\n",
        "        self.document_patterns = {}\n",
        "        self.value_contexts = defaultdict(list)\n",
        "        self.used_extractions = set()\n",
        "\n",
        "    def _get_engineering_units(self):\n",
        "        \"\"\"Essential engineering units - prevents garbage extraction\"\"\"\n",
        "        return {\n",
        "            'MPa': re.compile(r'(\\d+\\.?\\d*)\\s*MPa(?!\\w)', re.I),\n",
        "            'GPa': re.compile(r'(\\d+\\.?\\d*)\\s*GPa(?!\\w)', re.I),\n",
        "            'Pa': re.compile(r'(\\d+\\.?\\d*)\\s*Pa(?![a-zA-Z])', re.I),\n",
        "            '%': re.compile(r'(\\d+\\.?\\d*)\\s*%(?!\\w)'),\n",
        "            's⁻¹': re.compile(r'(\\d+\\.?\\d*(?:[×x]10[-−]?\\d+)?)\\s*s[-−]?1(?!\\w)', re.I),\n",
        "            '°C': re.compile(r'(\\d+\\.?\\d*)\\s*°C(?!\\w)'),\n",
        "            'J/m²': re.compile(r'(\\d+\\.?\\d*)\\s*J/m[²2](?!\\w)', re.I),\n",
        "            'g/cm³': re.compile(r'(\\d+\\.?\\d*)\\s*g/cm[³3](?!\\w)', re.I),\n",
        "            'dimensionless': re.compile(r'(?<!\\d)(\\d\\.\\d{2,4})(?!\\s*[A-Za-z%])')\n",
        "        }\n",
        "\n",
        "    def analyze_document_content(self, pdf_file):\n",
        "        \"\"\"Analyze what's actually in the document\"\"\"\n",
        "        try:\n",
        "            doc = fitz.open(pdf_file.name)\n",
        "            full_text = \"\"\n",
        "\n",
        "            for page in doc:\n",
        "                full_text += page.get_text()\n",
        "\n",
        "            doc.close()\n",
        "\n",
        "            # Learn what's actually available\n",
        "            available_data = self._discover_available_data(full_text)\n",
        "            document_type = self._classify_document_type(full_text)\n",
        "\n",
        "            return {\n",
        "                'text': full_text,\n",
        "                'available_data': available_data,\n",
        "                'document_type': document_type,\n",
        "                'data_summary': self._create_data_summary(available_data)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {'error': f\"Document analysis failed: {e}\"}\n",
        "\n",
        "    def _discover_available_data(self, text: str) -> List[Dict]:\n",
        "        \"\"\"Discover what numerical data is actually available\"\"\"\n",
        "        available_data = []\n",
        "        lines = text.split('\\n')\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            line_clean = line.strip()\n",
        "            if not line_clean:\n",
        "                continue\n",
        "\n",
        "            # Skip structural elements\n",
        "            if self._is_document_structure(line_clean):\n",
        "                continue\n",
        "\n",
        "            # Extract values with engineering units\n",
        "            for unit, pattern in self.engineering_units.items():\n",
        "                for match in pattern.finditer(line_clean):\n",
        "                    try:\n",
        "                        value_str = match.group(1)\n",
        "                        value = float(value_str.replace('×', 'e').replace('−', '-'))\n",
        "\n",
        "                        if self._is_meaningful_value(value, unit, line_clean):\n",
        "                            context = self._get_context(lines, i)\n",
        "                            data_type = self._identify_data_type(context)\n",
        "\n",
        "                            available_data.append({\n",
        "                                'value': value,\n",
        "                                'unit': unit,\n",
        "                                'context': line_clean,\n",
        "                                'full_context': context,\n",
        "                                'data_type': data_type,\n",
        "                                'line_number': i,\n",
        "                                'quality': self._assess_quality(context, unit)\n",
        "                            })\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "        return available_data\n",
        "\n",
        "    def _is_document_structure(self, line: str) -> bool:\n",
        "        \"\"\"Filter document structure elements\"\"\"\n",
        "        filters = [\n",
        "            r'^\\d+\\.\\s*[A-Z]',  # Section headers\n",
        "            r'©.*\\d{4}',  # Copyright\n",
        "            r'published by',  # Publication info\n",
        "            r'^\\s*\\[\\d+\\]',  # References\n",
        "            r'^(Abstract|Introduction|Conclusion|References)$'  # Section titles\n",
        "        ]\n",
        "\n",
        "        return any(re.search(pattern, line, re.I) for pattern in filters)\n",
        "\n",
        "    def _is_meaningful_value(self, value: float, unit: str, context: str) -> bool:\n",
        "        \"\"\"Check if value is meaningful (not composition)\"\"\"\n",
        "        if value <= 0 or value > 1e8:\n",
        "            return False\n",
        "\n",
        "        # Filter out material compositions\n",
        "        context_lower = context.lower()\n",
        "        if any(phrase in context_lower for phrase in [\n",
        "            'containing', 'composed of', 'copolymer', 'wt%', 'vol%'\n",
        "        ]):\n",
        "            return False\n",
        "\n",
        "        # Basic reasonableness\n",
        "        if unit == 'dimensionless' and value > 100:\n",
        "            return False\n",
        "        elif unit == '%' and 'containing' in context_lower:\n",
        "            return False  # Material composition\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _get_context(self, lines: List[str], center: int) -> str:\n",
        "        \"\"\"Get context around line\"\"\"\n",
        "        start = max(0, center - 2)\n",
        "        end = min(len(lines), center + 3)\n",
        "        return ' '.join([line.strip() for line in lines[start:end] if line.strip()])\n",
        "\n",
        "    def _identify_data_type(self, context: str) -> str:\n",
        "        \"\"\"Identify type of data\"\"\"\n",
        "        context_lower = context.lower()\n",
        "\n",
        "        if any(word in context_lower for word in ['table', 'data', 'values']):\n",
        "            return 'tabular_data'\n",
        "        elif any(word in context_lower for word in ['measured', 'tested', 'experimental']):\n",
        "            return 'experimental_data'\n",
        "        elif any(word in context_lower for word in ['parameter', 'coefficient']):\n",
        "            return 'model_parameter'\n",
        "        elif any(word in context_lower for word in ['condition', 'speed', 'rate']):\n",
        "            return 'test_condition'\n",
        "        else:\n",
        "            return 'general_text'\n",
        "\n",
        "    def _assess_quality(self, context: str, unit: str) -> str:\n",
        "        \"\"\"Assess data quality\"\"\"\n",
        "        context_lower = context.lower()\n",
        "\n",
        "        if any(word in context_lower for word in ['table', 'data']):\n",
        "            return 'high'\n",
        "        elif any(word in context_lower for word in ['measured', 'result']):\n",
        "            return 'medium'\n",
        "        else:\n",
        "            return 'low'\n",
        "\n",
        "    def _classify_document_type(self, text: str) -> str:\n",
        "        \"\"\"Classify document type\"\"\"\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        if any(word in text_lower for word in ['model', 'equation', 'analysis', 'methodology']):\n",
        "            return 'research_paper'\n",
        "        elif any(word in text_lower for word in ['specification', 'datasheet', 'standard']):\n",
        "            return 'technical_specification'\n",
        "        else:\n",
        "            return 'technical_document'\n",
        "\n",
        "    def _create_data_summary(self, available_data: List[Dict]) -> Dict:\n",
        "        \"\"\"Create summary of available data\"\"\"\n",
        "        summary = {\n",
        "            'total_data_points': len(available_data),\n",
        "            'units_found': list(set([d['unit'] for d in available_data])),\n",
        "            'data_types': dict(Counter([d['data_type'] for d in available_data])),\n",
        "            'quality_distribution': dict(Counter([d['quality'] for d in available_data]))\n",
        "        }\n",
        "        return summary\n",
        "\n",
        "    def match_data_to_properties(self, available_data: List[Dict], excel_properties: List[str]) -> Dict:\n",
        "        \"\"\"Match available data to Excel properties realistically\"\"\"\n",
        "        results = {}\n",
        "        used_data = set()\n",
        "\n",
        "        # Sort properties by likelihood of finding data\n",
        "        prioritized_props = self._prioritize_properties(excel_properties)\n",
        "\n",
        "        for prop_name in prioritized_props:\n",
        "            best_match = None\n",
        "            best_score = 0\n",
        "\n",
        "            for data in available_data:\n",
        "                data_key = (data['value'], data['unit'])\n",
        "                if data_key in used_data:\n",
        "                    continue\n",
        "\n",
        "                # Score match between data and property\n",
        "                score = self._score_data_property_match(data, prop_name)\n",
        "\n",
        "                if score > best_score and score >= 50:  # Reasonable threshold\n",
        "                    best_match = data\n",
        "                    best_score = score\n",
        "\n",
        "            if best_match and best_score >= 50:\n",
        "                used_data.add((best_match['value'], best_match['unit']))\n",
        "\n",
        "                results[prop_name] = {\n",
        "                    'value': f\"{best_match['value']} {best_match['unit']}\",\n",
        "                    'unit': best_match['unit'],\n",
        "                    'source': f\"Page data - {best_match['data_type']}\",\n",
        "                    'confidence': best_match['quality'],\n",
        "                    'score': best_score,\n",
        "                    'context': best_match['context'][:100],\n",
        "                    'data_type': best_match['data_type']\n",
        "                }\n",
        "            else:\n",
        "                results[prop_name] = {\n",
        "                    'value': 'N/A',\n",
        "                    'unit': 'N/A',\n",
        "                    'source': 'Not found in document',\n",
        "                    'confidence': 'none',\n",
        "                    'score': best_score,\n",
        "                    'reason': 'Data may not exist in this document type'\n",
        "                }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _prioritize_properties(self, properties: List[str]) -> List[str]:\n",
        "        \"\"\"Prioritize properties by likelihood of finding in research papers\"\"\"\n",
        "        def get_priority(prop: str) -> int:\n",
        "            prop_lower = prop.lower()\n",
        "            priority = 0\n",
        "\n",
        "            # Higher priority for properties likely in research papers\n",
        "            if any(word in prop_lower for word in ['stress', 'yield', 'modulus']):\n",
        "                priority += 20\n",
        "            if any(word in prop_lower for word in ['tensile', 'test', 'condition']):\n",
        "                priority += 15\n",
        "            if any(word in prop_lower for word in ['temperature', 'rate']):\n",
        "                priority += 10\n",
        "\n",
        "            return priority\n",
        "\n",
        "        return sorted(properties, key=get_priority, reverse=True)\n",
        "\n",
        "    def _score_data_property_match(self, data: Dict, property_name: str) -> float:\n",
        "        \"\"\"Score how well data matches property\"\"\"\n",
        "        score = 0\n",
        "        prop_lower = property_name.lower()\n",
        "        context_lower = data['context'].lower()\n",
        "\n",
        "        # Unit appropriateness\n",
        "        if self._is_unit_appropriate(data['unit'], prop_lower):\n",
        "            score += 30\n",
        "\n",
        "        # Keyword matching\n",
        "        prop_keywords = [word for word in prop_lower.split() if len(word) > 3]\n",
        "        keyword_matches = sum(1 for word in prop_keywords if word in context_lower)\n",
        "        score += keyword_matches * 15\n",
        "\n",
        "        # Data type appropriateness\n",
        "        if data['data_type'] in ['tabular_data', 'experimental_data']:\n",
        "            score += 20\n",
        "        elif data['data_type'] == 'model_parameter':\n",
        "            score += 15\n",
        "\n",
        "        # Quality bonus\n",
        "        if data['quality'] == 'high':\n",
        "            score += 15\n",
        "        elif data['quality'] == 'medium':\n",
        "            score += 10\n",
        "\n",
        "        return score\n",
        "\n",
        "    def _is_unit_appropriate(self, unit: str, property_name: str) -> bool:\n",
        "        \"\"\"Check if unit is appropriate for property\"\"\"\n",
        "        if 'modulus' in property_name:\n",
        "            return unit in ['GPa', 'MPa']\n",
        "        elif any(word in property_name for word in ['strength', 'stress']):\n",
        "            return unit in ['MPa', 'GPa']\n",
        "        elif any(word in property_name for word in ['strain', 'elongation']):\n",
        "            return unit in ['%', 'dimensionless']\n",
        "        elif 'temperature' in property_name:\n",
        "            return unit in ['°C']\n",
        "        elif 'rate' in property_name:\n",
        "            return unit in ['s⁻¹']\n",
        "        elif 'energy' in property_name:\n",
        "            return unit in ['J/m²']\n",
        "\n",
        "        return True\n",
        "\n",
        "    def extract_with_ai_assistance(self, pdf_text: str, target_properties: List[str]) -> Dict:\n",
        "        \"\"\"Use AI to extract specific property data\"\"\"\n",
        "        try:\n",
        "            # Focus on properties most likely to be in document\n",
        "            realistic_props = [p for p in target_properties[:10] if self._is_realistic_for_research_paper(p)]\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "EXTRACT SPECIFIC PROPERTY DATA FROM RESEARCH DOCUMENT\n",
        "\n",
        "TARGET PROPERTIES (only extract if explicitly mentioned):\n",
        "{json.dumps(realistic_props, indent=1)}\n",
        "\n",
        "DOCUMENT EXCERPT:\n",
        "{pdf_text[:8000]}\n",
        "\n",
        "CRITICAL INSTRUCTIONS:\n",
        "1. Only extract explicit numerical values with engineering units (MPa, GPa, %, °C, s⁻¹)\n",
        "2. Distinguish material composition from properties:\n",
        "   - \"containing 8% ethylene\" = material composition (DO NOT extract as property)\n",
        "   - \"yield stress 38 MPa\" = property data (CAN extract)\n",
        "3. Prefer experimental data and measurements over model parameters\n",
        "4. Each value should be used for only one property\n",
        "\n",
        "RESPONSE FORMAT:\n",
        "{{\n",
        "  \"PropertyName\": {{\n",
        "    \"value\": \"number unit\",\n",
        "    \"unit\": \"unit_only\",\n",
        "    \"source\": \"description\",\n",
        "    \"confidence\": \"high/medium/low\",\n",
        "    \"context\": \"where_found\"\n",
        "  }}\n",
        "}}\n",
        "\n",
        "Return only confident extractions in valid JSON.\n",
        "\"\"\"\n",
        "\n",
        "            response = requests.post(\n",
        "                f\"{GEMINI_ENDPOINT}?key={GEMINI_API_KEY}\",\n",
        "                json={\n",
        "                    \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
        "                    \"generationConfig\": {\"temperature\": 0.0, \"maxOutputTokens\": 1200}\n",
        "                },\n",
        "                timeout=60\n",
        "            )\n",
        "\n",
        "            if response.ok:\n",
        "                result = response.json()\n",
        "                content = result['candidates'][0]['content']['parts'][0]['text']\n",
        "                return self._parse_json_response(content)\n",
        "\n",
        "            return {}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"AI extraction failed: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def _is_realistic_for_research_paper(self, prop: str) -> bool:\n",
        "        \"\"\"Check if property is realistic to find in research papers\"\"\"\n",
        "        prop_lower = prop.lower()\n",
        "\n",
        "        # Properties often found in research papers\n",
        "        realistic_indicators = [\n",
        "            'stress', 'strain', 'modulus', 'yield', 'temperature',\n",
        "            'rate', 'speed', 'parameter'\n",
        "        ]\n",
        "\n",
        "        return any(indicator in prop_lower for indicator in realistic_indicators)\n",
        "\n",
        "    def _parse_json_response(self, content: str) -> Dict:\n",
        "        \"\"\"Parse AI JSON response\"\"\"\n",
        "        try:\n",
        "            if '```json' in content:\n",
        "                content = content.split('```json')[1].split('```')[0]\n",
        "\n",
        "            start = content.find('{')\n",
        "            end = content.rfind('}') + 1\n",
        "\n",
        "            if start != -1 and end > start:\n",
        "                return json.loads(content[start:end])\n",
        "            return {}\n",
        "        except:\n",
        "            return {}\n",
        "\n",
        "    def read_excel_properties(self, excel_file) -> List[str]:\n",
        "        \"\"\"Read Excel properties flexibly\"\"\"\n",
        "        try:\n",
        "            df = None\n",
        "            for header in [None, 0, 1]:\n",
        "                try:\n",
        "                    df = pd.read_excel(excel_file, header=header)\n",
        "                    if len(df) >= 3:\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if df is None:\n",
        "                return []\n",
        "\n",
        "            properties = []\n",
        "            for _, row in df.iterrows():\n",
        "                for cell in row:\n",
        "                    if pd.notna(cell):\n",
        "                        prop = str(cell).strip()\n",
        "                        if (len(prop) > 3 and\n",
        "                            prop.lower() not in ['description', 'value', 'unit', 'note'] and\n",
        "                            not prop.lower().startswith('unnamed')):\n",
        "                            properties.append(prop)\n",
        "                            break\n",
        "\n",
        "            return properties[:30]  # Reasonable limit\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Excel reading error: {e}\")\n",
        "            return []\n",
        "\n",
        "# Initialize extractor\n",
        "extractor = PracticalExtractor()\n",
        "\n",
        "# Global state\n",
        "document_data = {}\n",
        "excel_properties = []\n",
        "extraction_results = {}\n",
        "\n",
        "def analyze_documents(pdf_file, excel_file):\n",
        "    \"\"\"Analyze what's actually in the documents\"\"\"\n",
        "    global document_data, excel_properties\n",
        "\n",
        "    if not pdf_file or not excel_file:\n",
        "        return \"Upload both files\", \"No files provided\"\n",
        "\n",
        "    try:\n",
        "        # Analyze PDF content\n",
        "        document_data = extractor.analyze_document_content(pdf_file)\n",
        "\n",
        "        if 'error' in document_data:\n",
        "            return document_data['error'], \"PDF analysis failed\"\n",
        "\n",
        "        # Read Excel properties\n",
        "        excel_properties = extractor.read_excel_properties(excel_file)\n",
        "\n",
        "        if not excel_properties:\n",
        "            return \"Could not read Excel properties\", \"Excel reading failed\"\n",
        "\n",
        "        # Create realistic summary\n",
        "        available_data = document_data.get('available_data', [])\n",
        "        data_summary = document_data.get('data_summary', {})\n",
        "\n",
        "        analysis_text = f\"DOCUMENT ANALYSIS:\\n\"\n",
        "        analysis_text += f\"Document Type: {document_data.get('document_type', 'unknown')}\\n\"\n",
        "        analysis_text += f\"Available Data Points: {data_summary.get('total_data_points', 0)}\\n\"\n",
        "        analysis_text += f\"Engineering Units Found: {len(data_summary.get('units_found', []))}\\n\"\n",
        "        analysis_text += f\"Data Types: {data_summary.get('data_types', {})}\\n\"\n",
        "        analysis_text += f\"Quality Distribution: {data_summary.get('quality_distribution', {})}\\n\"\n",
        "        analysis_text += f\"Excel Properties: {len(excel_properties)}\\n\"\n",
        "\n",
        "        # Show available data points\n",
        "        preview_text = \"AVAILABLE NUMERICAL DATA:\\n\"\n",
        "        for i, dp in enumerate(available_data[:15]):\n",
        "            preview_text += f\"{i+1}. {dp['value']} {dp['unit']} - {dp['data_type']}\\n\"\n",
        "            preview_text += f\"   Context: {dp['context'][:80]}...\\n\\n\"\n",
        "\n",
        "        return preview_text, analysis_text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Analysis error: {e}\", \"Document analysis failed\"\n",
        "\n",
        "def extract_realistic_matches():\n",
        "    \"\"\"Extract realistic property matches\"\"\"\n",
        "    global document_data, excel_properties, extraction_results\n",
        "\n",
        "    if not document_data or not excel_properties:\n",
        "        return pd.DataFrame([{\"Error\": \"Analyze documents first\"}]), \"No analysis data\", \"\"\n",
        "\n",
        "    try:\n",
        "        available_data = document_data.get('available_data', [])\n",
        "\n",
        "        # Match available data to Excel properties\n",
        "        rule_based_results = extractor.match_data_to_properties(available_data, excel_properties)\n",
        "\n",
        "        # Enhance with AI for missing high-priority properties\n",
        "        ai_results = extractor.extract_with_ai_assistance(document_data.get('text', ''), excel_properties)\n",
        "\n",
        "        # Merge results (AI fills gaps)\n",
        "        for prop_name, ai_data in ai_results.items():\n",
        "            if rule_based_results.get(prop_name, {}).get('value') == 'N/A':\n",
        "                rule_based_results[prop_name] = ai_data\n",
        "\n",
        "        extraction_results = rule_based_results\n",
        "\n",
        "        # Create results table\n",
        "        table_rows = []\n",
        "        for prop_name in excel_properties:\n",
        "            data = rule_based_results.get(prop_name, {})\n",
        "            table_rows.append({\n",
        "                \"Property\": prop_name,\n",
        "                \"Value\": data.get('value', 'N/A'),\n",
        "                \"Unit\": data.get('unit', 'N/A'),\n",
        "                \"Source\": data.get('source', 'N/A'),\n",
        "                \"Confidence\": data.get('confidence', 'none'),\n",
        "                \"Score\": f\"{data.get('score', 0):.1f}\",\n",
        "                \"Data Type\": data.get('data_type', 'N/A'),\n",
        "                \"Context\": data.get('context', data.get('reason', 'N/A'))\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(table_rows)\n",
        "\n",
        "        # Realistic statistics\n",
        "        found_count = len([r for r in rule_based_results.values() if r.get('value') != 'N/A'])\n",
        "        high_conf_count = len([r for r in rule_based_results.values() if r.get('confidence') == 'high'])\n",
        "\n",
        "        status_text = f\"REALISTIC EXTRACTION RESULTS:\\n\"\n",
        "        status_text += f\"Document Type: {document_data.get('document_type')}\\n\"\n",
        "        status_text += f\"Available Data Points: {len(available_data)}\\n\"\n",
        "        status_text += f\"Excel Properties Requested: {len(excel_properties)}\\n\"\n",
        "        status_text += f\"Successful Matches: {found_count}\\n\"\n",
        "        status_text += f\"High Confidence Matches: {high_conf_count}\\n\"\n",
        "        status_text += f\"Success Rate: {(found_count/len(excel_properties)*100):.1f}%\\n\"\n",
        "        status_text += f\"\\nREALITY CHECK: Research papers typically contain only a subset of standard material properties.\\n\"\n",
        "        status_text += f\"Many N/A results are expected and correct for this document type.\"\n",
        "\n",
        "        # JSON output\n",
        "        json_data = {\n",
        "            \"extraction_metadata\": {\n",
        "                \"approach\": \"realistic_document_driven_extraction\",\n",
        "                \"document_type\": document_data.get('document_type'),\n",
        "                \"total_properties\": len(excel_properties),\n",
        "                \"found_values\": found_count,\n",
        "                \"success_rate\": f\"{(found_count/len(excel_properties)*100):.1f}%\",\n",
        "                \"timestamp\": datetime.now().isoformat()\n",
        "            },\n",
        "            \"available_document_data\": available_data[:10],\n",
        "            \"extraction_results\": rule_based_results\n",
        "        }\n",
        "\n",
        "        json_output = json.dumps(json_data, indent=2)\n",
        "\n",
        "        return df, status_text, json_output\n",
        "\n",
        "    except Exception as e:\n",
        "        error_df = pd.DataFrame([{\"Error\": f\"Extraction failed: {str(e)}\"}])\n",
        "        return error_df, f\"Error: {str(e)}\", \"\"\n",
        "\n",
        "def download_realistic_results():\n",
        "    \"\"\"Download results\"\"\"\n",
        "    global extraction_results, excel_properties\n",
        "\n",
        "    if not extraction_results:\n",
        "        return None\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"realistic_extraction_{timestamp}.json\"\n",
        "\n",
        "    output = {\n",
        "        \"realistic_extraction\": {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"approach\": \"document_content_driven\",\n",
        "            \"methodology\": \"extract_available_not_expected\"\n",
        "        },\n",
        "        \"results\": extraction_results\n",
        "    }\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(output, f, indent=2)\n",
        "\n",
        "    return filename\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks(title=\"Practical Property Extractor\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # Practical Property Extractor\n",
        "    **Extracts what's actually available in your documents**\n",
        "\n",
        "    This system analyzes your document type and extracts available data realistically.\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Tab(\"Document Analysis\"):\n",
        "        with gr.Row():\n",
        "            pdf_input = gr.File(label=\"PDF Document\", file_types=[\".pdf\"])\n",
        "            excel_input = gr.File(label=\"Excel Properties\", file_types=[\".xlsx\", \".xls\"])\n",
        "\n",
        "        analyze_btn = gr.Button(\"Analyze Available Data\", variant=\"primary\")\n",
        "\n",
        "        with gr.Row():\n",
        "            available_preview = gr.Textbox(label=\"Available Data Points\", lines=15)\n",
        "            analysis_summary = gr.Textbox(label=\"Document Analysis\", lines=15)\n",
        "\n",
        "    with gr.Tab(\"Realistic Extraction\"):\n",
        "        extract_btn = gr.Button(\"Extract Available Properties\", variant=\"secondary\", size=\"lg\")\n",
        "\n",
        "        results_table = gr.Dataframe(\n",
        "            label=\"Realistic Results\",\n",
        "            headers=[\"Property\", \"Value\", \"Unit\", \"Source\", \"Confidence\", \"Score\", \"Data Type\", \"Context\"]\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            status_output = gr.Textbox(label=\"Realistic Status\", lines=12)\n",
        "            json_output = gr.Code(label=\"Results JSON\", language=\"json\", lines=10)\n",
        "\n",
        "    with gr.Tab(\"Download\"):\n",
        "        download_btn = gr.Button(\"Download Results\")\n",
        "        file_output = gr.File(label=\"Results File\")\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        **Realistic Approach:**\n",
        "        - Analyzes document type and content realistically\n",
        "        - Extracts data that actually exists\n",
        "        - Honest about document limitations\n",
        "        - Filters out material composition descriptions\n",
        "        - Focuses on experimental data and model parameters\n",
        "        - Provides realistic success expectations\n",
        "        \"\"\")\n",
        "\n",
        "    # Event handlers\n",
        "    analyze_btn.click(\n",
        "        analyze_documents,\n",
        "        inputs=[pdf_input, excel_input],\n",
        "        outputs=[available_preview, analysis_summary]\n",
        "    )\n",
        "\n",
        "    extract_btn.click(\n",
        "        extract_realistic_matches,\n",
        "        outputs=[results_table, status_output, json_output]\n",
        "    )\n",
        "\n",
        "    download_btn.click(\n",
        "        download_realistic_results,\n",
        "        outputs=[file_output]\n",
        "    )\n",
        "\n",
        "print(\"Practical Property Extractor ready!\")\n",
        "demo.launch(debug=True, share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8E8ZrKWdIXO"
      },
      "outputs": [],
      "source": [
        "# Practical Adaptive Property Extractor\n",
        "# Essential engineering knowledge + Document-specific learning\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Quick setup for Colab\n",
        "def quick_setup():\n",
        "    try:\n",
        "        import fitz\n",
        "        import gradio as gr\n",
        "    except ImportError:\n",
        "        packages = [\"PyMuPDF==1.23.26\", \"gradio\", \"requests\", \"pandas\", \"numpy\", \"Pillow\"]\n",
        "        for package in packages:\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], capture_output=True)\n",
        "\n",
        "quick_setup()\n",
        "\n",
        "import re\n",
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, List, Any, Tuple, Optional\n",
        "from collections import defaultdict, Counter\n",
        "from datetime import datetime\n",
        "import base64\n",
        "import io\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import fitz\n",
        "\n",
        "# API Configuration\n",
        "GEMINI_API_KEY = \"AIzaSyCFzlJFsIq6PYLuHSPqLYvg0clx-CPpSD0\"\n",
        "GEMINI_ENDPOINT = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent\"\n",
        "\n",
        "class PracticalExtractor:\n",
        "    def __init__(self):\n",
        "        # Essential: Valid engineering units (prevents \"NPL\", \"Modelling\" extraction)\n",
        "        self.engineering_units = self._get_engineering_units()\n",
        "\n",
        "        # Adaptive: Learned from document\n",
        "        self.document_patterns = {}\n",
        "        self.value_contexts = defaultdict(list)\n",
        "        self.used_extractions = set()\n",
        "\n",
        "    def _get_engineering_units(self):\n",
        "        \"\"\"Essential engineering units - prevents garbage extraction\"\"\"\n",
        "        return {\n",
        "            'MPa': re.compile(r'(\\d+\\.?\\d*)\\s*MPa(?!\\w)', re.I),\n",
        "            'GPa': re.compile(r'(\\d+\\.?\\d*)\\s*GPa(?!\\w)', re.I),\n",
        "            'Pa': re.compile(r'(\\d+\\.?\\d*)\\s*Pa(?![a-zA-Z])', re.I),\n",
        "            '%': re.compile(r'(\\d+\\.?\\d*)\\s*%(?!\\w)'),\n",
        "            's⁻¹': re.compile(r'(\\d+\\.?\\d*(?:[×x]10[-−]?\\d+)?)\\s*s[-−]?1(?!\\w)', re.I),\n",
        "            '°C': re.compile(r'(\\d+\\.?\\d*)\\s*°C(?!\\w)'),\n",
        "            'J/m²': re.compile(r'(\\d+\\.?\\d*)\\s*J/m[²2](?!\\w)', re.I),\n",
        "            'g/cm³': re.compile(r'(\\d+\\.?\\d*)\\s*g/cm[³3](?!\\w)', re.I),\n",
        "            'dimensionless': re.compile(r'(?<!\\d)(\\d\\.\\d{2,4})(?!\\s*[A-Za-z%])')\n",
        "        }\n",
        "\n",
        "    def analyze_document_content(self, pdf_file):\n",
        "        \"\"\"Analyze what's actually in the document\"\"\"\n",
        "        try:\n",
        "            doc = fitz.open(pdf_file.name)\n",
        "            full_text = \"\"\n",
        "\n",
        "            for page in doc:\n",
        "                full_text += page.get_text()\n",
        "\n",
        "            doc.close()\n",
        "\n",
        "            # Learn what's actually available\n",
        "            available_data = self._discover_available_data(full_text)\n",
        "            document_type = self._classify_document_type(full_text)\n",
        "\n",
        "            return {\n",
        "                'text': full_text,\n",
        "                'available_data': available_data,\n",
        "                'document_type': document_type,\n",
        "                'data_summary': self._create_data_summary(available_data)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {'error': f\"Document analysis failed: {e}\"}\n",
        "\n",
        "    def _discover_available_data(self, text: str) -> List[Dict]:\n",
        "        \"\"\"Discover what numerical data is actually available\"\"\"\n",
        "        available_data = []\n",
        "        lines = text.split('\\n')\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            line_clean = line.strip()\n",
        "            if not line_clean:\n",
        "                continue\n",
        "\n",
        "            # Skip structural elements\n",
        "            if self._is_document_structure(line_clean):\n",
        "                continue\n",
        "\n",
        "            # Extract values with engineering units\n",
        "            for unit, pattern in self.engineering_units.items():\n",
        "                for match in pattern.finditer(line_clean):\n",
        "                    try:\n",
        "                        value_str = match.group(1)\n",
        "                        value = float(value_str.replace('×', 'e').replace('−', '-'))\n",
        "\n",
        "                        if self._is_meaningful_value(value, unit, line_clean):\n",
        "                            context = self._get_context(lines, i)\n",
        "                            data_type = self._identify_data_type(context)\n",
        "\n",
        "                            available_data.append({\n",
        "                                'value': value,\n",
        "                                'unit': unit,\n",
        "                                'context': line_clean,\n",
        "                                'full_context': context,\n",
        "                                'data_type': data_type,\n",
        "                                'line_number': i,\n",
        "                                'quality': self._assess_quality(context, unit)\n",
        "                            })\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "        return available_data\n",
        "\n",
        "    def _is_document_structure(self, line: str) -> bool:\n",
        "        \"\"\"Filter document structure elements\"\"\"\n",
        "        filters = [\n",
        "            r'^\\d+\\.\\s*[A-Z]',  # Section headers\n",
        "            r'©.*\\d{4}',  # Copyright\n",
        "            r'published by',  # Publication info\n",
        "            r'^\\s*\\[\\d+\\]',  # References\n",
        "            r'^(Abstract|Introduction|Conclusion|References)$'  # Section titles\n",
        "        ]\n",
        "\n",
        "        return any(re.search(pattern, line, re.I) for pattern in filters)\n",
        "\n",
        "    def _is_meaningful_value(self, value: float, unit: str, context: str) -> bool:\n",
        "        \"\"\"Check if value is meaningful (not composition)\"\"\"\n",
        "        if value <= 0 or value > 1e8:\n",
        "            return False\n",
        "\n",
        "        # Filter out material compositions\n",
        "        context_lower = context.lower()\n",
        "        if any(phrase in context_lower for phrase in [\n",
        "            'containing', 'composed of', 'copolymer', 'wt%', 'vol%'\n",
        "        ]):\n",
        "            return False\n",
        "\n",
        "        # Basic reasonableness\n",
        "        if unit == 'dimensionless' and value > 100:\n",
        "            return False\n",
        "        elif unit == '%' and 'containing' in context_lower:\n",
        "            return False  # Material composition\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _get_context(self, lines: List[str], center: int) -> str:\n",
        "        \"\"\"Get context around line\"\"\"\n",
        "        start = max(0, center - 2)\n",
        "        end = min(len(lines), center + 3)\n",
        "        return ' '.join([line.strip() for line in lines[start:end] if line.strip()])\n",
        "\n",
        "    def _identify_data_type(self, context: str) -> str:\n",
        "        \"\"\"Identify type of data\"\"\"\n",
        "        context_lower = context.lower()\n",
        "\n",
        "        if any(word in context_lower for word in ['table', 'data', 'values']):\n",
        "            return 'tabular_data'\n",
        "        elif any(word in context_lower for word in ['measured', 'tested', 'experimental']):\n",
        "            return 'experimental_data'\n",
        "        elif any(word in context_lower for word in ['parameter', 'coefficient']):\n",
        "            return 'model_parameter'\n",
        "        elif any(word in context_lower for word in ['condition', 'speed', 'rate']):\n",
        "            return 'test_condition'\n",
        "        else:\n",
        "            return 'general_text'\n",
        "\n",
        "    def _assess_quality(self, context: str, unit: str) -> str:\n",
        "        \"\"\"Assess data quality\"\"\"\n",
        "        context_lower = context.lower()\n",
        "\n",
        "        if any(word in context_lower for word in ['table', 'data']):\n",
        "            return 'high'\n",
        "        elif any(word in context_lower for word in ['measured', 'result']):\n",
        "            return 'medium'\n",
        "        else:\n",
        "            return 'low'\n",
        "\n",
        "    def _classify_document_type(self, text: str) -> str:\n",
        "        \"\"\"Classify document type\"\"\"\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        if any(word in text_lower for word in ['model', 'equation', 'analysis', 'methodology']):\n",
        "            return 'research_paper'\n",
        "        elif any(word in text_lower for word in ['specification', 'datasheet', 'standard']):\n",
        "            return 'technical_specification'\n",
        "        else:\n",
        "            return 'technical_document'\n",
        "\n",
        "    def _create_data_summary(self, available_data: List[Dict]) -> Dict:\n",
        "        \"\"\"Create summary of available data\"\"\"\n",
        "        summary = {\n",
        "            'total_data_points': len(available_data),\n",
        "            'units_found': list(set([d['unit'] for d in available_data])),\n",
        "            'data_types': dict(Counter([d['data_type'] for d in available_data])),\n",
        "            'quality_distribution': dict(Counter([d['quality'] for d in available_data]))\n",
        "        }\n",
        "        return summary\n",
        "\n",
        "    def match_data_to_properties(self, available_data: List[Dict], excel_properties: List[str]) -> Dict:\n",
        "        \"\"\"Match available data to Excel properties realistically\"\"\"\n",
        "        results = {}\n",
        "        used_data = set()\n",
        "\n",
        "        # Sort properties by likelihood of finding data\n",
        "        prioritized_props = self._prioritize_properties(excel_properties)\n",
        "\n",
        "        for prop_name in prioritized_props:\n",
        "            best_match = None\n",
        "            best_score = 0\n",
        "\n",
        "            for data in available_data:\n",
        "                data_key = (data['value'], data['unit'])\n",
        "                if data_key in used_data:\n",
        "                    continue\n",
        "\n",
        "                # Score match between data and property\n",
        "                score = self._score_data_property_match(data, prop_name)\n",
        "\n",
        "                if score > best_score and score >= 50:  # Reasonable threshold\n",
        "                    best_match = data\n",
        "                    best_score = score\n",
        "\n",
        "            if best_match and best_score >= 50:\n",
        "                used_data.add((best_match['value'], best_match['unit']))\n",
        "\n",
        "                results[prop_name] = {\n",
        "                    'value': f\"{best_match['value']} {best_match['unit']}\",\n",
        "                    'unit': best_match['unit'],\n",
        "                    'source': f\"Page data - {best_match['data_type']}\",\n",
        "                    'confidence': best_match['quality'],\n",
        "                    'score': best_score,\n",
        "                    'context': best_match['context'][:100],\n",
        "                    'data_type': best_match['data_type']\n",
        "                }\n",
        "            else:\n",
        "                results[prop_name] = {\n",
        "                    'value': 'N/A',\n",
        "                    'unit': 'N/A',\n",
        "                    'source': 'Not found in document',\n",
        "                    'confidence': 'none',\n",
        "                    'score': best_score,\n",
        "                    'reason': 'Data may not exist in this document type'\n",
        "                }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _prioritize_properties(self, properties: List[str]) -> List[str]:\n",
        "        \"\"\"Prioritize properties by likelihood of finding in research papers\"\"\"\n",
        "        def get_priority(prop: str) -> int:\n",
        "            prop_lower = prop.lower()\n",
        "            priority = 0\n",
        "\n",
        "            # Higher priority for properties likely in research papers\n",
        "            if any(word in prop_lower for word in ['stress', 'yield', 'modulus']):\n",
        "                priority += 20\n",
        "            if any(word in prop_lower for word in ['tensile', 'test', 'condition']):\n",
        "                priority += 15\n",
        "            if any(word in prop_lower for word in ['temperature', 'rate']):\n",
        "                priority += 10\n",
        "\n",
        "            return priority\n",
        "\n",
        "        return sorted(properties, key=get_priority, reverse=True)\n",
        "\n",
        "    def _score_data_property_match(self, data: Dict, property_name: str) -> float:\n",
        "        \"\"\"Score how well data matches property\"\"\"\n",
        "        score = 0\n",
        "        prop_lower = property_name.lower()\n",
        "        context_lower = data['context'].lower()\n",
        "\n",
        "        # Unit appropriateness\n",
        "        if self._is_unit_appropriate(data['unit'], prop_lower):\n",
        "            score += 30\n",
        "\n",
        "        # Keyword matching\n",
        "        prop_keywords = [word for word in prop_lower.split() if len(word) > 3]\n",
        "        keyword_matches = sum(1 for word in prop_keywords if word in context_lower)\n",
        "        score += keyword_matches * 15\n",
        "\n",
        "        # Data type appropriateness\n",
        "        if data['data_type'] in ['tabular_data', 'experimental_data']:\n",
        "            score += 20\n",
        "        elif data['data_type'] == 'model_parameter':\n",
        "            score += 15\n",
        "\n",
        "        # Quality bonus\n",
        "        if data['quality'] == 'high':\n",
        "            score += 15\n",
        "        elif data['quality'] == 'medium':\n",
        "            score += 10\n",
        "\n",
        "        return score\n",
        "\n",
        "    def _is_unit_appropriate(self, unit: str, property_name: str) -> bool:\n",
        "        \"\"\"Check if unit is appropriate for property\"\"\"\n",
        "        if 'modulus' in property_name:\n",
        "            return unit in ['GPa', 'MPa']\n",
        "        elif any(word in property_name for word in ['strength', 'stress']):\n",
        "            return unit in ['MPa', 'GPa']\n",
        "        elif any(word in property_name for word in ['strain', 'elongation']):\n",
        "            return unit in ['%', 'dimensionless']\n",
        "        elif 'temperature' in property_name:\n",
        "            return unit in ['°C']\n",
        "        elif 'rate' in property_name:\n",
        "            return unit in ['s⁻¹']\n",
        "        elif 'energy' in property_name:\n",
        "            return unit in ['J/m²']\n",
        "\n",
        "        return True\n",
        "\n",
        "    def extract_with_ai_assistance(self, pdf_text: str, target_properties: List[str]) -> Dict:\n",
        "        \"\"\"Use AI to extract specific property data\"\"\"\n",
        "        try:\n",
        "            # Focus on properties most likely to be in document\n",
        "            realistic_props = [p for p in target_properties[:10] if self._is_realistic_for_research_paper(p)]\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "EXTRACT SPECIFIC PROPERTY DATA FROM RESEARCH DOCUMENT\n",
        "\n",
        "TARGET PROPERTIES (only extract if explicitly mentioned):\n",
        "{json.dumps(realistic_props, indent=1)}\n",
        "\n",
        "DOCUMENT EXCERPT:\n",
        "{pdf_text[:8000]}\n",
        "\n",
        "CRITICAL INSTRUCTIONS:\n",
        "1. Only extract explicit numerical values with engineering units (MPa, GPa, %, °C, s⁻¹)\n",
        "2. Distinguish material composition from properties:\n",
        "   - \"containing 8% ethylene\" = material composition (DO NOT extract as property)\n",
        "   - \"yield stress 38 MPa\" = property data (CAN extract)\n",
        "3. Prefer experimental data and measurements over model parameters\n",
        "4. Each value should be used for only one property\n",
        "\n",
        "RESPONSE FORMAT:\n",
        "{{\n",
        "  \"PropertyName\": {{\n",
        "    \"value\": \"number unit\",\n",
        "    \"unit\": \"unit_only\",\n",
        "    \"source\": \"description\",\n",
        "    \"confidence\": \"high/medium/low\",\n",
        "    \"context\": \"where_found\"\n",
        "  }}\n",
        "}}\n",
        "\n",
        "Return only confident extractions in valid JSON.\n",
        "\"\"\"\n",
        "\n",
        "            response = requests.post(\n",
        "                f\"{GEMINI_ENDPOINT}?key={GEMINI_API_KEY}\",\n",
        "                json={\n",
        "                    \"contents\": [{\"parts\": [{\"text\": prompt}]}],\n",
        "                    \"generationConfig\": {\"temperature\": 0.0, \"maxOutputTokens\": 1200}\n",
        "                },\n",
        "                timeout=60\n",
        "            )\n",
        "\n",
        "            if response.ok:\n",
        "                result = response.json()\n",
        "                content = result['candidates'][0]['content']['parts'][0]['text']\n",
        "                return self._parse_json_response(content)\n",
        "\n",
        "            return {}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"AI extraction failed: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def _is_realistic_for_research_paper(self, prop: str) -> bool:\n",
        "        \"\"\"Check if property is realistic to find in research papers\"\"\"\n",
        "        prop_lower = prop.lower()\n",
        "\n",
        "        # Properties often found in research papers\n",
        "        realistic_indicators = [\n",
        "            'stress', 'strain', 'modulus', 'yield', 'temperature',\n",
        "            'rate', 'speed', 'parameter'\n",
        "        ]\n",
        "\n",
        "        return any(indicator in prop_lower for indicator in realistic_indicators)\n",
        "\n",
        "    def _parse_json_response(self, content: str) -> Dict:\n",
        "        \"\"\"Parse AI JSON response\"\"\"\n",
        "        try:\n",
        "            if '```json' in content:\n",
        "                content = content.split('```json')[1].split('```')[0]\n",
        "\n",
        "            start = content.find('{')\n",
        "            end = content.rfind('}') + 1\n",
        "\n",
        "            if start != -1 and end > start:\n",
        "                return json.loads(content[start:end])\n",
        "            return {}\n",
        "        except:\n",
        "            return {}\n",
        "\n",
        "    def read_excel_properties(self, excel_file) -> List[str]:\n",
        "        \"\"\"Read Excel properties flexibly\"\"\"\n",
        "        try:\n",
        "            df = None\n",
        "            for header in [None, 0, 1]:\n",
        "                try:\n",
        "                    df = pd.read_excel(excel_file, header=header)\n",
        "                    if len(df) >= 3:\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            if df is None:\n",
        "                return []\n",
        "\n",
        "            properties = []\n",
        "            for _, row in df.iterrows():\n",
        "                for cell in row:\n",
        "                    if pd.notna(cell):\n",
        "                        prop = str(cell).strip()\n",
        "                        if (len(prop) > 3 and\n",
        "                            prop.lower() not in ['description', 'value', 'unit', 'note'] and\n",
        "                            not prop.lower().startswith('unnamed')):\n",
        "                            properties.append(prop)\n",
        "                            break\n",
        "\n",
        "            return properties[:30]  # Reasonable limit\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Excel reading error: {e}\")\n",
        "            return []\n",
        "\n",
        "# Initialize extractor\n",
        "extractor = PracticalExtractor()\n",
        "\n",
        "# Global state\n",
        "document_data = {}\n",
        "excel_properties = []\n",
        "extraction_results = {}\n",
        "\n",
        "def analyze_documents(pdf_file, excel_file):\n",
        "    \"\"\"Analyze what's actually in the documents\"\"\"\n",
        "    global document_data, excel_properties\n",
        "\n",
        "    if not pdf_file or not excel_file:\n",
        "        return \"Upload both files\", \"No files provided\"\n",
        "\n",
        "    try:\n",
        "        # Analyze PDF content\n",
        "        document_data = extractor.analyze_document_content(pdf_file)\n",
        "\n",
        "        if 'error' in document_data:\n",
        "            return document_data['error'], \"PDF analysis failed\"\n",
        "\n",
        "        # Read Excel properties\n",
        "        excel_properties = extractor.read_excel_properties(excel_file)\n",
        "\n",
        "        if not excel_properties:\n",
        "            return \"Could not read Excel properties\", \"Excel reading failed\"\n",
        "\n",
        "        # Create realistic summary\n",
        "        available_data = document_data.get('available_data', [])\n",
        "        data_summary = document_data.get('data_summary', {})\n",
        "\n",
        "        analysis_text = f\"DOCUMENT ANALYSIS:\\n\"\n",
        "        analysis_text += f\"Document Type: {document_data.get('document_type', 'unknown')}\\n\"\n",
        "        analysis_text += f\"Available Data Points: {data_summary.get('total_data_points', 0)}\\n\"\n",
        "        analysis_text += f\"Engineering Units Found: {len(data_summary.get('units_found', []))}\\n\"\n",
        "        analysis_text += f\"Data Types: {data_summary.get('data_types', {})}\\n\"\n",
        "        analysis_text += f\"Quality Distribution: {data_summary.get('quality_distribution', {})}\\n\"\n",
        "        analysis_text += f\"Excel Properties: {len(excel_properties)}\\n\"\n",
        "\n",
        "        # Show available data points\n",
        "        preview_text = \"AVAILABLE NUMERICAL DATA:\\n\"\n",
        "        for i, dp in enumerate(available_data[:15]):\n",
        "            preview_text += f\"{i+1}. {dp['value']} {dp['unit']} - {dp['data_type']}\\n\"\n",
        "            preview_text += f\"   Context: {dp['context'][:80]}...\\n\\n\"\n",
        "\n",
        "        return preview_text, analysis_text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Analysis error: {e}\", \"Document analysis failed\"\n",
        "\n",
        "def extract_realistic_matches():\n",
        "    \"\"\"Extract realistic property matches\"\"\"\n",
        "    global document_data, excel_properties, extraction_results\n",
        "\n",
        "    if not document_data or not excel_properties:\n",
        "        return pd.DataFrame([{\"Error\": \"Analyze documents first\"}]), \"No analysis data\", \"\"\n",
        "\n",
        "    try:\n",
        "        available_data = document_data.get('available_data', [])\n",
        "\n",
        "        # Match available data to Excel properties\n",
        "        rule_based_results = extractor.match_data_to_properties(available_data, excel_properties)\n",
        "\n",
        "        # Enhance with AI for missing high-priority properties\n",
        "        ai_results = extractor.extract_with_ai_assistance(document_data.get('text', ''), excel_properties)\n",
        "\n",
        "        # Merge results (AI fills gaps)\n",
        "        for prop_name, ai_data in ai_results.items():\n",
        "            if rule_based_results.get(prop_name, {}).get('value') == 'N/A':\n",
        "                rule_based_results[prop_name] = ai_data\n",
        "\n",
        "        extraction_results = rule_based_results\n",
        "\n",
        "        # Create results table\n",
        "        table_rows = []\n",
        "        for prop_name in excel_properties:\n",
        "            data = rule_based_results.get(prop_name, {})\n",
        "            table_rows.append({\n",
        "                \"Property\": prop_name,\n",
        "                \"Value\": data.get('value', 'N/A'),\n",
        "                \"Unit\": data.get('unit', 'N/A'),\n",
        "                \"Source\": data.get('source', 'N/A'),\n",
        "                \"Confidence\": data.get('confidence', 'none'),\n",
        "                \"Score\": f\"{data.get('score', 0):.1f}\",\n",
        "                \"Data Type\": data.get('data_type', 'N/A'),\n",
        "                \"Context\": data.get('context', data.get('reason', 'N/A'))\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(table_rows)\n",
        "\n",
        "        # Realistic statistics\n",
        "        found_count = len([r for r in rule_based_results.values() if r.get('value') != 'N/A'])\n",
        "        high_conf_count = len([r for r in rule_based_results.values() if r.get('confidence') == 'high'])\n",
        "\n",
        "        status_text = f\"REALISTIC EXTRACTION RESULTS:\\n\"\n",
        "        status_text += f\"Document Type: {document_data.get('document_type')}\\n\"\n",
        "        status_text += f\"Available Data Points: {len(available_data)}\\n\"\n",
        "        status_text += f\"Excel Properties Requested: {len(excel_properties)}\\n\"\n",
        "        status_text += f\"Successful Matches: {found_count}\\n\"\n",
        "        status_text += f\"High Confidence Matches: {high_conf_count}\\n\"\n",
        "        status_text += f\"Success Rate: {(found_count/len(excel_properties)*100):.1f}%\\n\"\n",
        "        status_text += f\"\\nREALITY CHECK: Research papers typically contain only a subset of standard material properties.\\n\"\n",
        "        status_text += f\"Many N/A results are expected and correct for this document type.\"\n",
        "\n",
        "        # JSON output\n",
        "        json_data = {\n",
        "            \"extraction_metadata\": {\n",
        "                \"approach\": \"realistic_document_driven_extraction\",\n",
        "                \"document_type\": document_data.get('document_type'),\n",
        "                \"total_properties\": len(excel_properties),\n",
        "                \"found_values\": found_count,\n",
        "                \"success_rate\": f\"{(found_count/len(excel_properties)*100):.1f}%\",\n",
        "                \"timestamp\": datetime.now().isoformat()\n",
        "            },\n",
        "            \"available_document_data\": available_data[:10],\n",
        "            \"extraction_results\": rule_based_results\n",
        "        }\n",
        "\n",
        "        json_output = json.dumps(json_data, indent=2)\n",
        "\n",
        "        return df, status_text, json_output\n",
        "\n",
        "    except Exception as e:\n",
        "        error_df = pd.DataFrame([{\"Error\": f\"Extraction failed: {str(e)}\"}])\n",
        "        return error_df, f\"Error: {str(e)}\", \"\"\n",
        "\n",
        "def download_realistic_results():\n",
        "    \"\"\"Download results\"\"\"\n",
        "    global extraction_results, excel_properties\n",
        "\n",
        "    if not extraction_results:\n",
        "        return None\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"realistic_extraction_{timestamp}.json\"\n",
        "\n",
        "    output = {\n",
        "        \"realistic_extraction\": {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"approach\": \"document_content_driven\",\n",
        "            \"methodology\": \"extract_available_not_expected\"\n",
        "        },\n",
        "        \"results\": extraction_results\n",
        "    }\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(output, f, indent=2)\n",
        "\n",
        "    return filename\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks(title=\"Practical Property Extractor\") as demo:\n",
        "    gr.Markdown(\"\"\"\n",
        "    # Practical Property Extractor\n",
        "    **Extracts what's actually available in your documents**\n",
        "\n",
        "    This system analyzes your document type and extracts available data realistically.\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Tab(\"Document Analysis\"):\n",
        "        with gr.Row():\n",
        "            pdf_input = gr.File(label=\"PDF Document\", file_types=[\".pdf\"])\n",
        "            excel_input = gr.File(label=\"Excel Properties\", file_types=[\".xlsx\", \".xls\"])\n",
        "\n",
        "        analyze_btn = gr.Button(\"Analyze Available Data\", variant=\"primary\")\n",
        "\n",
        "        with gr.Row():\n",
        "            available_preview = gr.Textbox(label=\"Available Data Points\", lines=15)\n",
        "            analysis_summary = gr.Textbox(label=\"Document Analysis\", lines=15)\n",
        "\n",
        "    with gr.Tab(\"Realistic Extraction\"):\n",
        "        extract_btn = gr.Button(\"Extract Available Properties\", variant=\"secondary\", size=\"lg\")\n",
        "\n",
        "        results_table = gr.Dataframe(\n",
        "            label=\"Realistic Results\",\n",
        "            headers=[\"Property\", \"Value\", \"Unit\", \"Source\", \"Confidence\", \"Score\", \"Data Type\", \"Context\"]\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            status_output = gr.Textbox(label=\"Realistic Status\", lines=12)\n",
        "            json_output = gr.Code(label=\"Results JSON\", language=\"json\", lines=10)\n",
        "\n",
        "    with gr.Tab(\"Download\"):\n",
        "        download_btn = gr.Button(\"Download Results\")\n",
        "        file_output = gr.File(label=\"Results File\")\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        **Realistic Approach:**\n",
        "        - Analyzes document type and content realistically\n",
        "        - Extracts data that actually exists\n",
        "        - Honest about document limitations\n",
        "        - Filters out material composition descriptions\n",
        "        - Focuses on experimental data and model parameters\n",
        "        - Provides realistic success expectations\n",
        "        \"\"\")\n",
        "\n",
        "    # Event handlers\n",
        "    analyze_btn.click(\n",
        "        analyze_documents,\n",
        "        inputs=[pdf_input, excel_input],\n",
        "        outputs=[available_preview, analysis_summary]\n",
        "    )\n",
        "\n",
        "    extract_btn.click(\n",
        "        extract_realistic_matches,\n",
        "        outputs=[results_table, status_output, json_output]\n",
        "    )\n",
        "\n",
        "    download_btn.click(\n",
        "        download_realistic_results,\n",
        "        outputs=[file_output]\n",
        "    )\n",
        "\n",
        "print(\"Practical Property Extractor ready!\")\n",
        "demo.launch(debug=True, share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mp66bYeOgB_z"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "class AccurateExtractor:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def extract_from_pdf(self, pdf_path: str) -> dict:\n",
        "        \"\"\"\n",
        "        Extract values from PDF text dynamically.\n",
        "        \"\"\"\n",
        "        # Load all text from PDF\n",
        "        doc = fitz.open(pdf_path)\n",
        "        text = \"\\n\".join(page.get_text() for page in doc)\n",
        "        doc.close()\n",
        "\n",
        "        # --- Regex patterns ---\n",
        "        yield_matches = re.findall(r\"so\\s*\\(MPa\\)\\s*(?:\\d+\\.?\\d*\\s*)+\", text)\n",
        "        flow_matches  = re.findall(r\"sf\\s*\\(MPa\\)\\s*(?:\\d+\\.?\\d*\\s*)+\", text)\n",
        "        poisson_matches = re.findall(r\"Poisson.?s ratio.*?(\\d\\.\\d+)\", text)\n",
        "        strain_matches = re.findall(r\"ėp\\s*\\(s−1\\)\\s*((?:\\d+\\.?\\d*\\s*)+)\", text)\n",
        "\n",
        "        # --- Clean values ---\n",
        "        yield_vals = re.findall(r\"\\d+\\.?\\d*\", \" \".join(yield_matches)) if yield_matches else []\n",
        "        flow_vals  = re.findall(r\"\\d+\\.?\\d*\", \" \".join(flow_matches)) if flow_matches else []\n",
        "        strain_vals = re.findall(r\"\\d+\\.?\\d*\", \" \".join(strain_matches)) if strain_matches else []\n",
        "\n",
        "        return {\n",
        "            \"yield_range\": f\"{min(map(float,yield_vals))}–{max(map(float,yield_vals))}\" if yield_vals else \"N/A\",\n",
        "            \"flow_range\": f\"{min(map(float,flow_vals))}–{max(map(float,flow_vals))}\" if flow_vals else \"N/A\",\n",
        "            \"strain_range\": f\"{min(map(float,strain_vals))}–{max(map(float,strain_vals))}\" if strain_vals else \"N/A\",\n",
        "            \"poisson\": poisson_matches[0] if poisson_matches else \"N/A\"\n",
        "        }\n",
        "\n",
        "    def match_to_excel(self, extracted: dict, excel_properties: list) -> dict:\n",
        "        \"\"\"\n",
        "        Map extracted PDF values into Excel schema.\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "        for prop in excel_properties:\n",
        "            if not isinstance(prop, str):\n",
        "                continue\n",
        "            pl = prop.lower()\n",
        "            value, unit, source, conf = \"N/A\", \"N/A\", \"Not found\", \"none\"\n",
        "\n",
        "            if \"yield\" in pl and \"stress\" in pl:\n",
        "                value, unit, source, conf = extracted[\"yield_range\"], \"MPa\", \"Table 1\", \"high\"\n",
        "\n",
        "            elif (\"strength\" in pl or \"flow\" in pl) and \"stress\" in pl:\n",
        "                value, unit, source, conf = extracted[\"flow_range\"], \"MPa\", \"Table 1\", \"high\"\n",
        "\n",
        "            elif \"poisson\" in pl:\n",
        "                value, unit, source, conf = extracted[\"poisson\"], \"dimensionless\", \"Text/Table\", \"medium\"\n",
        "\n",
        "            elif \"strain rate\" in pl:\n",
        "                value, unit, source, conf = extracted[\"strain_range\"], \"s⁻¹\", \"Table 1\", \"high\"\n",
        "\n",
        "            elif \"elongation\" in pl or \"strain at break\" in pl:\n",
        "                value, unit, source, conf = \"N/A\", \"%\", \"Not reported\", \"none\"\n",
        "\n",
        "            elif \"modulus\" in pl:\n",
        "                value, unit, source, conf = \"N/A\", \"GPa\", \"Not reported\", \"none\"\n",
        "\n",
        "            results[prop] = {\n",
        "                \"value\": value,\n",
        "                \"unit\": unit,\n",
        "                \"source\": source,\n",
        "                \"confidence\": conf\n",
        "            }\n",
        "        return results\n",
        "\n",
        "    def fill_excel(self, excel_path: str, output_path: str, results: dict):\n",
        "        \"\"\"\n",
        "        Fill Excel file with results.\n",
        "        \"\"\"\n",
        "        wb = load_workbook(excel_path)\n",
        "        ws = wb.active\n",
        "\n",
        "        # Assume header row is 4 (Tensile | Description | Fixed Value | Unit ...)\n",
        "        header_row = 4\n",
        "        props_col = 1   # 'Tensile'\n",
        "        value_col = 3   # 'Fixed Value'\n",
        "        unit_col = 4    # 'Unit'\n",
        "\n",
        "        for row in range(header_row+1, ws.max_row+1):\n",
        "            prop = ws.cell(row=row, column=props_col).value\n",
        "            if prop and prop in results:\n",
        "                ws.cell(row=row, column=value_col).value = results[prop][\"value\"]\n",
        "                ws.cell(row=row, column=unit_col).value = results[prop][\"unit\"]\n",
        "\n",
        "        wb.save(output_path)\n",
        "        return output_path\n",
        "\n",
        "\n",
        "# ---------------- MAIN SCRIPT ----------------\n",
        "if __name__ == \"__main__\":\n",
        "    pdf_file = \"1-s2.0-S0142941801000034-main.pdf\"\n",
        "    excel_file = \"5.1__.xlsx\"\n",
        "    output_file = \"5.1_filled.xlsx\"\n",
        "\n",
        "    extractor = AccurateExtractor()\n",
        "\n",
        "    # Step 1: Extract PDF values\n",
        "    extracted = extractor.extract_from_pdf(pdf_file)\n",
        "    print(\"Extracted from PDF:\", extracted)\n",
        "\n",
        "    # Step 2: Load Excel properties\n",
        "    df = pd.read_excel(excel_file, header=4)\n",
        "    excel_props = df['Tensile stress at yield'].dropna().tolist()\n",
        "\n",
        "    # Step 3: Match\n",
        "    results = extractor.match_to_excel(extracted, excel_props)\n",
        "\n",
        "    # Step 4: Fill Excel\n",
        "    extractor.fill_excel(excel_file, output_file, results)\n",
        "    print(f\" Filled results saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poVbn9AunuEQ"
      },
      "outputs": [],
      "source": [
        "# Accurate Property Extractor - Section Aware\n",
        "# Focus on Scientific Tables and Context\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "def quick_setup():\n",
        "    try:\n",
        "        import fitz\n",
        "        import gradio as gr\n",
        "    except ImportError:\n",
        "        packages = [\"PyMuPDF==1.23.26\", \"gradio\", \"requests\", \"pandas\", \"numpy\", \"Pillow\"]\n",
        "        for package in packages:\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], capture_output=True)\n",
        "\n",
        "quick_setup()\n",
        "\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, List\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "import gradio as gr\n",
        "import fitz\n",
        "\n",
        "# API placeholders if you want LLM integration later\n",
        "GEMINI_API_KEY = \"AIzaSyCFzlJFsIq6PYLuHSPqLYvg0clx-CPpSD0\"\n",
        "GEMINI_ENDPOINT = \"https://generativelanguage.googleapis.com/v1/models/gemini-2.5-flash-latest:generateContent\"\n",
        "\n",
        "class AccurateExtractor:\n",
        "    def __init__(self):\n",
        "        self.engineering_units = self._get_enhanced_unit_patterns()\n",
        "\n",
        "        self.table_indicators = [\n",
        "            r'Table\\s+\\d+',\n",
        "            r'^\\s*\\d+\\.?\\d*\\s+\\d+\\.?\\d*\\s+\\d+\\.?\\d*',\n",
        "            r'strain\\s+rate.*values',\n",
        "            r'parameters.*equation',\n",
        "            r'σ[₀f].*MPa',\n",
        "        ]\n",
        "\n",
        "        self.property_patterns = {\n",
        "            'tensile_yield': [\n",
        "                r'σ[₀o].*?(\\d+(?:\\.\\d+)?(?:\\s*[-–]\\s*\\d+(?:\\.\\d+)?)?)\\s*MPa',\n",
        "                r'yield\\s+stress.*?(\\d+(?:\\.\\d+)?(?:\\s*[-–]\\s*\\d+(?:\\.\\d+)?)?)\\s*MPa',\n",
        "            ],\n",
        "            'tensile_strength': [\n",
        "                r'σ[f].*?(\\d+(?:\\.\\d+)?(?:\\s*[-–]\\s*\\d+(?:\\.\\d+)?)?)\\s*MPa',\n",
        "                r'tensile\\s+strength.*?(\\d+(?:\\.\\d+)?(?:\\s*[-–]\\s*\\d+(?:\\.\\d+)?)?)\\s*MPa',\n",
        "            ],\n",
        "            'strain_rate': [\n",
        "                r'(\\d+(?:\\.\\d+)?(?:[×x]10[-−]?\\d+)?)\\s*s[-−]¹',\n",
        "            ],\n",
        "            'poisson_ratio': [\n",
        "                r'Poisson.*?ratio.*?(\\d+\\.\\d+)',\n",
        "                r'ν.*?(\\d+\\.\\d+)',\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    def _get_enhanced_unit_patterns(self):\n",
        "        return {\n",
        "            'MPa': re.compile(r'(\\d+\\.?\\d*(?:\\s*[-–]\\s*\\d+\\.?\\d*)?)\\s*MPa(?!\\w)', re.I),\n",
        "            'GPa': re.compile(r'(\\d+\\.?\\d*(?:\\s*[-–]\\s*\\d+\\.?\\d*)?)\\s*GPa(?!\\w)', re.I),\n",
        "            's⁻¹': re.compile(r'(\\d+\\.?\\d*(?:[×x]10[-−]?\\d+)?)\\s*s[-−]?¹?(?!\\w)', re.I),\n",
        "            '%': re.compile(r'(\\d+\\.?\\d*)\\s*%(?!\\w)'),\n",
        "            '°C': re.compile(r'(\\d+\\.?\\d*)\\s*°C(?!\\w)'),\n",
        "            'dimensionless': re.compile(r'(?<!\\w)(\\d\\.\\d{2,4})(?!\\s*[A-Za-z%])')\n",
        "        }\n",
        "\n",
        "    # --- Section splitting ---\n",
        "    def _split_sections(self, full_text: str) -> Dict[str, str]:\n",
        "        sections = defaultdict(str)\n",
        "        current = \"general\"\n",
        "        for line in full_text.split(\"\\n\"):\n",
        "            l = line.lower()\n",
        "            if \"tensile\" in l or \"tension\" in l:\n",
        "                current = \"tensile\"\n",
        "            elif \"compression\" in l:\n",
        "                current = \"compression\"\n",
        "            elif \"shear\" in l:\n",
        "                current = \"shear\"\n",
        "            elif \"fracture\" in l:\n",
        "                current = \"fracture\"\n",
        "            elif \"flexural\" in l or \"bending\" in l:\n",
        "                current = \"flexural\"\n",
        "            elif \"impact\" in l or \"charpy\" in l or \"izod\" in l:\n",
        "                current = \"impact\"\n",
        "            sections[current] += line + \"\\n\"\n",
        "        return sections\n",
        "\n",
        "    def extract_tables_from_pdf(self, pdf_file):\n",
        "        try:\n",
        "            doc = fitz.open(pdf_file.name)\n",
        "            tables_data = []\n",
        "            full_text = \"\"\n",
        "            for page_num, page in enumerate(doc):\n",
        "                page_text = page.get_text()\n",
        "                full_text += page_text\n",
        "                lines = page_text.split('\\n')\n",
        "                tables_data.extend(self._identify_table_regions(lines, page_num))\n",
        "            doc.close()\n",
        "\n",
        "            parsed_tables = []\n",
        "            for table_info in tables_data:\n",
        "                parsed = self._parse_table_data(table_info)\n",
        "                if parsed:\n",
        "                    parsed_tables.append(parsed)\n",
        "\n",
        "            return {'full_text': full_text, 'tables': parsed_tables, 'table_count': len(parsed_tables)}\n",
        "        except Exception as e:\n",
        "            return {'error': f\"Table extraction failed: {e}\"}\n",
        "\n",
        "    def _identify_table_regions(self, lines, page_num):\n",
        "        tables, current_table = [], None\n",
        "        for i, line in enumerate(lines):\n",
        "            line_clean = line.strip()\n",
        "            if not line_clean:\n",
        "                continue\n",
        "            if any(re.search(pattern, line_clean, re.I) for pattern in self.table_indicators):\n",
        "                if current_table:\n",
        "                    tables.append(current_table)\n",
        "                current_table = {'start_line': i, 'page': page_num, 'header': line_clean,\n",
        "                                 'data_lines': [], 'type': self._classify_table_type(line_clean)}\n",
        "                continue\n",
        "            if current_table and self._is_data_line(line_clean):\n",
        "                current_table['data_lines'].append({\n",
        "                    'line_num': i,\n",
        "                    'content': line_clean,\n",
        "                    'values': self._extract_values_from_line(line_clean)\n",
        "                })\n",
        "            elif current_table and len(current_table['data_lines']) > 0:\n",
        "                tables.append(current_table)\n",
        "                current_table = None\n",
        "        if current_table:\n",
        "            tables.append(current_table)\n",
        "        return tables\n",
        "\n",
        "    def _is_data_line(self, line):\n",
        "        numbers = re.findall(r'\\d+\\.?\\d*', line)\n",
        "        if len(numbers) < 2:\n",
        "            return False\n",
        "        has_units = any(u in line for u in ['MPa', 'GPa', 's⁻¹', 's-1', '%', '°C'])\n",
        "        has_sci = any(p in line.lower() for p in ['e-', 'e+', '×10', 'x10'])\n",
        "        return has_units or has_sci or len(numbers) >= 3\n",
        "\n",
        "    def _extract_values_from_line(self, line):\n",
        "        values = []\n",
        "        for unit, pattern in self.engineering_units.items():\n",
        "            for match in pattern.finditer(line):\n",
        "                try:\n",
        "                    val = match.group(1)\n",
        "                    if '-' in val or '–' in val:\n",
        "                        values.append({'raw': val, 'unit': unit, 'type': 'range', 'context': line})\n",
        "                    else:\n",
        "                        values.append({'value': float(val), 'raw': val, 'unit': unit, 'type': 'single', 'context': line})\n",
        "                except:\n",
        "                    continue\n",
        "        return values\n",
        "\n",
        "    def _classify_table_type(self, header):\n",
        "        h = header.lower()\n",
        "        if 'strain rate' in h: return 'test_conditions'\n",
        "        if 'parameter' in h: return 'model_parameters'\n",
        "        return 'general'\n",
        "\n",
        "    def _parse_table_data(self, table_info):\n",
        "        if len(table_info['data_lines']) == 0: return None\n",
        "        parsed_data = {'table_type': table_info['type'], 'page': table_info['page'],\n",
        "                       'header': table_info['header'], 'data_points': []}\n",
        "        for d in table_info['data_lines']:\n",
        "            for v in d['values']:\n",
        "                parsed_data['data_points'].append({'value': v, 'line_context': d['content'],\n",
        "                                                   'quality': self._assess_data_quality(v, d['content'])})\n",
        "        return parsed_data\n",
        "\n",
        "    def _assess_data_quality(self, v, context):\n",
        "        score = 50\n",
        "        if 'table' in context.lower(): score += 20\n",
        "        if v['unit'] in ['MPa', 'GPa', 's⁻¹']: score += 15\n",
        "        if v['type'] == 'range': score += 10\n",
        "        if v['type'] == 'single' and (v['value'] < 0 or v['value'] > 10000): score -= 20\n",
        "        return min(100, max(0, score))\n",
        "\n",
        "    def match_extracted_data_to_properties(self, extracted_data, excel_props):\n",
        "        results = {}\n",
        "        sections = self._split_sections(extracted_data['full_text'])\n",
        "        all_points = [dp for t in extracted_data['tables'] for dp in t['data_points']]\n",
        "        all_points.sort(key=lambda x: x['quality'], reverse=True)\n",
        "        used = set()\n",
        "\n",
        "        for prop in excel_props:\n",
        "            best, best_score = None, 0\n",
        "            sec_key = \"general\"\n",
        "            pl = prop.lower()\n",
        "            if \"tensile\" in pl: sec_key = \"tensile\"\n",
        "            elif \"compression\" in pl: sec_key = \"compression\"\n",
        "            elif \"shear\" in pl: sec_key = \"shear\"\n",
        "            elif \"fracture\" in pl: sec_key = \"fracture\"\n",
        "            elif \"flexural\" in pl: sec_key = \"flexural\"\n",
        "            elif \"impact\" in pl: sec_key = \"impact\"\n",
        "\n",
        "            matches = self._find_property_specific_matches(prop, sections.get(sec_key, \"\"))\n",
        "            if matches:\n",
        "                best, best_score = matches[0], 90\n",
        "            else:\n",
        "                for dp in all_points:\n",
        "                    key = (str(dp['value']), dp['line_context'])\n",
        "                    if key in used: continue\n",
        "                    score = self._score_property_match(dp, prop)\n",
        "                    if score > best_score and score >= 60:\n",
        "                        best, best_score = dp, score\n",
        "\n",
        "            if best:\n",
        "                used.add((str(best.get('value')), best.get('line_context', '')))\n",
        "                results[prop] = self._format_result(best, best_score)\n",
        "            else:\n",
        "                results[prop] = {\"value\": \"N/A\", \"unit\": \"N/A\", \"source\": \"Not found\",\n",
        "                                 \"confidence\": \"none\", \"score\": 0}\n",
        "        return results\n",
        "\n",
        "    def _find_property_specific_matches(self, prop, section_text):\n",
        "        prop_lower = prop.lower()\n",
        "        matches, patterns = [], []\n",
        "        if \"yield\" in prop_lower: patterns = self.property_patterns['tensile_yield']\n",
        "        elif \"strength\" in prop_lower: patterns = self.property_patterns['tensile_strength']\n",
        "        elif \"strain rate\" in prop_lower: patterns = self.property_patterns['strain_rate']\n",
        "        elif \"poisson\" in prop_lower: patterns = self.property_patterns['poisson_ratio']\n",
        "\n",
        "        for pat in patterns:\n",
        "            for match in re.finditer(pat, section_text, re.I):\n",
        "                try:\n",
        "                    val = match.group(1)\n",
        "                    ctx = self._get_match_context(section_text, match.start(), match.end())\n",
        "                    matches.append({'value_str': val, 'context': ctx, 'pattern_matched': pat, 'quality': 85})\n",
        "                except:\n",
        "                    continue\n",
        "        return sorted(matches, key=lambda x: x['quality'], reverse=True)\n",
        "\n",
        "    def _get_match_context(self, text, start, end):\n",
        "        return text[max(0, start - 80): min(len(text), end + 80)].strip()\n",
        "\n",
        "    def _score_property_match(self, dp, prop):\n",
        "        score, pl = 0, prop.lower()\n",
        "        v, ctx = dp['value'], dp['line_context'].lower()\n",
        "        if self._is_unit_appropriate_enhanced(v['unit'], pl): score += 40\n",
        "        else: return 0\n",
        "        score += sum(15 for w in pl.split() if len(w) > 3 and w in ctx)\n",
        "        score += min(dp['quality'] * 0.3, 30)\n",
        "        return score\n",
        "\n",
        "    def _is_unit_appropriate_enhanced(self, unit, prop):\n",
        "        unit_map = {\n",
        "            'modulus': ['GPa', 'MPa'], 'stress': ['MPa', 'GPa'], 'strength': ['MPa', 'GPa'],\n",
        "            'yield': ['MPa', 'GPa'], 'strain': ['%', 'dimensionless'], 'elongation': ['%', 'dimensionless'],\n",
        "            'rate': ['s⁻¹'], 'temperature': ['°C'], 'poisson': ['dimensionless'], 'ratio': ['dimensionless']\n",
        "        }\n",
        "        return any(k in prop and unit in v for k, v in unit_map.items())\n",
        "\n",
        "    def _format_result(self, match, score):\n",
        "        if isinstance(match, dict) and 'value_str' in match:\n",
        "            return {\"value\": match['value_str'], \"unit\": self._infer_unit_from_context(match['context']),\n",
        "                    \"source\": \"Pattern match\", \"confidence\": \"high\" if score >= 80 else \"medium\",\n",
        "                    \"score\": score, \"context\": match['context'][:80]}\n",
        "        v = match['value']\n",
        "        return {\"value\": v.get('raw', str(v.get('value', 'N/A'))), \"unit\": v['unit'],\n",
        "                \"source\": \"Table extraction\", \"confidence\": \"high\" if score >= 80 else \"medium\",\n",
        "                \"score\": score, \"context\": match['line_context'][:80]}\n",
        "\n",
        "    def _infer_unit_from_context(self, ctx):\n",
        "        if 'MPa' in ctx: return 'MPa'\n",
        "        if 'GPa' in ctx: return 'GPa'\n",
        "        if 's' in ctx: return 's⁻¹'\n",
        "        if '%' in ctx: return '%'\n",
        "        return 'dimensionless'\n",
        "\n",
        "    def read_excel_properties(self, excel_file):\n",
        "        df = pd.read_excel(excel_file, header=None)\n",
        "        props = []\n",
        "        for _, row in df.iterrows():\n",
        "            for c in row:\n",
        "                if pd.notna(c):\n",
        "                    val = str(c).strip()\n",
        "                    if len(val) > 3 and not val.lower().startswith(('description', 'value', 'unit', 'unnamed')):\n",
        "                        props.append(val); break\n",
        "        return props[:50]\n",
        "\n",
        "# ---- Gradio UI ----\n",
        "extractor = AccurateExtractor()\n",
        "document_data, excel_props, extraction_results = {}, [], {}\n",
        "\n",
        "def analyze_pdf_tables(pdf_file, excel_file):\n",
        "    global document_data, excel_props\n",
        "    document_data = extractor.extract_tables_from_pdf(pdf_file)\n",
        "    excel_props = extractor.read_excel_properties(excel_file)\n",
        "    if 'error' in document_data: return document_data['error'], \"Analysis failed\"\n",
        "    tables = document_data.get('tables', [])\n",
        "    txt = f\"Tables: {len(tables)}\\nExcel props: {len(excel_props)}\"\n",
        "    return \"Preview ready\", txt\n",
        "\n",
        "def extract_accurate_properties():\n",
        "    global document_data, excel_props, extraction_results\n",
        "    extraction_results = extractor.match_extracted_data_to_properties(document_data, excel_props)\n",
        "    rows = [{\"Property\": p, \"Value\": d.get('value'), \"Unit\": d.get('unit'),\n",
        "             \"Source\": d.get('source'), \"Confidence\": d.get('confidence'),\n",
        "             \"Score\": d.get('score')} for p, d in extraction_results.items()]\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df, f\"Extracted {len(rows)} props\", json.dumps(extraction_results, indent=2)\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Tab(\"Analyze\"):\n",
        "        pdf_input = gr.File(label=\"PDF\", file_types=[\".pdf\"])\n",
        "        excel_input = gr.File(label=\"Excel\", file_types=[\".xlsx\"])\n",
        "        analyze_btn = gr.Button(\"Analyze\")\n",
        "        out1, out2 = gr.Textbox(), gr.Textbox()\n",
        "        analyze_btn.click(analyze_pdf_tables, [pdf_input, excel_input], [out1, out2])\n",
        "    with gr.Tab(\"Extract\"):\n",
        "        extract_btn = gr.Button(\"Extract Properties\")\n",
        "        df_out, stat, js_out = gr.Dataframe(), gr.Textbox(), gr.Code(language=\"json\")\n",
        "        extract_btn.click(extract_accurate_properties, [], [df_out, stat, js_out])\n",
        "\n",
        "print(\" Section-aware Accurate Property Extractor ready\")\n",
        "demo.launch(debug=True, share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7X7XakVKqGvO"
      },
      "outputs": [],
      "source": [
        "pip install PyPDF2"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPftGO5PiV4gSbZk21EmDXm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}